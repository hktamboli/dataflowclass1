{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce35fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.runners import DataflowRunner\n",
    "# Set up Apache Beam pipeline options.\n",
    "options = pipeline_options.PipelineOptions()\n",
    "\n",
    "# Set the project to the default project in your current Google Cloud\n",
    "# environment.\n",
    "_, options.view_as(GoogleCloudOptions).project = google.auth.default()\n",
    "\n",
    "# Set the Google Cloud region to run Dataflow.\n",
    "options.view_as(GoogleCloudOptions).region = 'us-central1'\n",
    "\n",
    "# Choose a Cloud Storage location.\n",
    "dataflow_gcs_location = 'gs://<change me>/dataflow'\n",
    "\n",
    "# Set the staging location. This location is used to stage the\n",
    "# Dataflow pipeline and SDK binary.\n",
    "options.view_as(GoogleCloudOptions).staging_location = '%s/staging' % dataflow_gcs_location\n",
    "\n",
    "# Set the temporary location. This location is used to store temporary files\n",
    "# or intermediate results before outputting to the sink.\n",
    "options.view_as(GoogleCloudOptions).temp_location = '%s/temp' % dataflow_gcs_location\n",
    "\n",
    "# Set the SDK location. This is used by Dataflow to locate the\n",
    "# SDK needed to run the pipeline.\n",
    "options.view_as(pipeline_options.SetupOptions).sdk_location = (\n",
    "    '/root/apache-beam-custom/packages/beam/sdks/python/dist/apache-beam-%s0.tar.gz' %\n",
    "    beam.version.__version__)\n",
    "runner = DataflowRunner()\n",
    "runner.run_pipeline(p, options=options)\n",
    "\n",
    "  with beam.Pipeline(options=pipeline_options) as p:\n",
    "    regions = (\n",
    "        p | 'Read Regions' >> ReadFromText(f'{known_args.input}/regions.csv')\n",
    "          | 'Parse Regions' >> beam.ParDo(RegionParseDict())\n",
    "    )\n",
    "\n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText(f'{known_args.input}/territories.csv')\n",
    "          | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "    )\n",
    "\n",
    "    lookup = (\n",
    "        territories\n",
    "        | beam.ParDo(LookupRegion(), lookuptable = beam.pvalue.AsList(regions))\n",
    "        | 'Write' >> WriteToText(f'{known_args.output}/sideinputs.csv')\n",
    "    )\n",
    "    p.run()\n",
    "\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f2db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# python sideinput.py --runner Dataflowrunner --project $PROJECT --region us-central1 --temp_location gs://$PROJECT/tmp\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import re\n",
    "\n",
    "#from past.builtins import unicode\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "class RegionParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid': int(regionid), 'regionname': regionname.title()}\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "        \n",
    "                \n",
    "class LookupRegion(beam.DoFn):\n",
    "    def process(self, element, lookuptable = [{'regionid':1, 'regionname':'North'}, {'regionid':2, 'regionname':'South'}]):\n",
    "        territoryid, territoryname, regionid = element\n",
    "        # Becase the regions PCollection is a different shape, use the following comprehension to make it easier to do a lookup\n",
    "        lookup = {e['regionid'] : e['regionname'] for e in lookuptable }\n",
    "        yield(territoryid, territoryname, regionid, lookup.get(regionid, 'No Region'))\n",
    "\n",
    "def run(argv=None, save_main_session=True):\n",
    "  \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--input',\n",
    "      dest='input',\n",
    "      default='gs://qwiklabs-gcp-04-c21b49858f60',\n",
    "      help='Input file to process.')\n",
    "  parser.add_argument(\n",
    "      '--output',\n",
    "      dest='output',\n",
    "      default = 'gs://qwiklabs-gcp-04-c21b49858f60/regions_output',      \n",
    "      help='Output file to write results to.')\n",
    "  known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "\n",
    "  # We use the save_main_session option because one or more DoFn's in this\n",
    "  # workflow rely on global context (e.g., a module imported at module level).\n",
    "  pipeline_options = PipelineOptions(pipeline_args)\n",
    "  pipeline_options.view_as(SetupOptions).save_main_session = save_main_session\n",
    "\n",
    "  # The pipeline will be run on exiting the with block.\n",
    "  with beam.Pipeline(options=pipeline_options) as p:\n",
    "    regions = (\n",
    "        p | 'Read Regions' >> ReadFromText(f'{known_args.input}/regions.csv')\n",
    "          | 'Parse Regions' >> beam.ParDo(RegionParseDict())\n",
    "    )\n",
    "\n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText(f'{known_args.input}/territories.csv')\n",
    "          | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "    )\n",
    "\n",
    "    lookup = (\n",
    "        territories\n",
    "        | beam.ParDo(LookupRegion(), lookuptable = beam.pvalue.AsList(regions))\n",
    "        | 'Write' >> WriteToText(f'{known_args.output}/sideinputs.csv')\n",
    "    )\n",
    "    p.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  logging.getLogger().setLevel(logging.INFO)\n",
    "  run()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Beam 2.28.0 for Python 3",
   "language": "python",
   "name": "apache-beam-2.28.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
