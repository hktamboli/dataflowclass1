{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4b5ce6c-55cb-45b7-9ad1-370ee54bf5c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install a Spark docker using the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835ac48-b14e-4e01-8290-b9799ba6e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker pull bitnami/spark && \\\n",
    "docker network create spark_network && \\\n",
    "docker run -d --name spark --network=spark_network -e SPARK_MODE=master bitnami/spark\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d659dd20-a9ab-4008-8bce-a5ff5acf5690",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fdb46e-7eef-4af7-bd41-587e716e3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pip\n",
    "\n",
    "def install(package):\n",
    "    if hasattr(pip, 'main'):\n",
    "        pip.main(['install', package])\n",
    "    else:\n",
    "        pip._internal.main(['install', package])\n",
    "\n",
    "install('pyspark')\n",
    "        \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2140ae18-4293-4560-91d6-4324e41fb0b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initialize the Spark context variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6573258c-97d5-4282-9278-cfe9ad82a186",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing pyspark\n",
      "pyspark initialized\n",
      "<SparkContext master=local[*] appName=Notebook> <pyspark.sql.session.SparkSession object at 0x7fbc44c16b90>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def initspark(appname = \"Notebook\", servername = \"local[*]\"):\n",
    "    print ('initializing pyspark')\n",
    "    conf = SparkConf().setAppName(appname).setMaster(servername)\n",
    "    sc = SparkContext(conf=conf)\n",
    "    spark = SparkSession.builder.appName(appname).enableHiveSupport().getOrCreate()\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "    print ('pyspark initialized')\n",
    "    return sc, spark, conf\n",
    "\n",
    "sc, spark, conf = initspark()\n",
    "print(sc, spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbca6a8-f8cc-45ac-8a62-0d14bfb665d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Initialize helper functions to run Java inside cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79d334c3-e064-4241-aa70-26d939b3e166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/github/apache/beam/blob/master/examples/notebooks/get-started/try-apache-beam-java.ipynb#scrollTo=CgTXBdTsBn1F\n",
    "# Run and print a shell command.\n",
    "def run(cmd):\n",
    "  print('>> {}'.format(cmd))\n",
    "  !{cmd}  # This is magic to run 'cmd' in the shell.\n",
    "  print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be2cc1a8-3e8e-428c-a69f-a3e51b402b57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> /opt/gradle-5.0/bin/gradle --console=plain -v\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "\u001b[m\n",
      "------------------------------------------------------------\n",
      "Gradle 5.0\n",
      "------------------------------------------------------------\n",
      "\n",
      "Build time:   2018-11-26 11:48:43 UTC\n",
      "Revision:     7fc6e5abf2fc5fe0824aec8a0f5462664dbcd987\n",
      "\n",
      "Kotlin DSL:   1.0.4\n",
      "Kotlin:       1.3.10\n",
      "Groovy:       2.5.4\n",
      "Ant:          Apache Ant(TM) version 1.9.13 compiled on July 10 2018\n",
      "JVM:          1.8.0_92 (Azul Systems, Inc. 25.92-b15)\n",
      "OS:           Linux 4.19.0-18-cloud-amd64 amd64\n",
      "\n",
      "\u001b[m\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Download the gradle source.\n",
    "gradle_version = 'gradle-5.0'\n",
    "gradle_path = f\"/opt/{gradle_version}\"\n",
    "if not os.path.exists(gradle_path):\n",
    "  run(f\"wget -q -nc -O gradle.zip https://services.gradle.org/distributions/{gradle_version}-bin.zip\")\n",
    "  run('unzip -q -d /opt gradle.zip')\n",
    "  run('rm -f gradle.zip')\n",
    "\n",
    "# We're choosing to use the absolute path instead of adding it to the $PATH environment variable.\n",
    "def gradle(args):\n",
    "  run(f\"{gradle_path}/bin/gradle --console=plain {args}\")\n",
    "\n",
    "gradle('-v')\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a716a7d4-28ea-40e7-90db-3e762b7dbcf9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Definition for %%java Python magic cell function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a97925b-df54-4700-bb8b-d4a3e2305b99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_magic, register_cell_magic, register_line_cell_magic\n",
    "@register_cell_magic\n",
    "def java(line, cell):\n",
    "    \"\"\"\n",
    "    Written by Joseph Gagliardo Jr.\n",
    "    joegagliardo@gmail.com\n",
    "    2021-12-22\n",
    "    \"\"\"\n",
    "    text = \"\"\"\n",
    "plugins {\n",
    "  // id 'idea'     // Uncomment for IntelliJ IDE\n",
    "  // id 'eclipse'  // Uncomment for Eclipse IDE\n",
    "\n",
    "  // Apply java plugin and make it a runnable application.\n",
    "  id 'java'\n",
    "  id 'application'\n",
    "\n",
    "  // 'shadow' allows us to embed all the dependencies into a fat jar.\n",
    "  id 'com.github.johnrengelman.shadow' version '4.0.3'\n",
    "}\n",
    "\n",
    "// This is the path of the main class, stored within ./src/main/java/\n",
    "mainClassName = 'samples.quickstart.{class_name}'\n",
    "\n",
    "// Declare the sources from which to fetch dependencies.\n",
    "repositories {\n",
    "  mavenCentral()\n",
    "}\n",
    "\n",
    "// Java version compatibility.\n",
    "sourceCompatibility = 1.8\n",
    "targetCompatibility = 1.8\n",
    "\n",
    "// Use the latest Apache Beam major version 2.\n",
    "// You can also lock into a minor version like '2.9.+'.\n",
    "ext.apacheBeamVersion = '2.+'\n",
    "\n",
    "// Declare the dependencies of the project.\n",
    "dependencies {\n",
    "  shadow \"org.apache.beam:beam-sdks-java-core:$apacheBeamVersion\"\n",
    "\n",
    "  runtime \"org.apache.beam:beam-runners-direct-java:$apacheBeamVersion\"\n",
    "  runtime \"org.slf4j:slf4j-api:1.+\"\n",
    "  runtime \"org.slf4j:slf4j-jdk14:1.+\"\n",
    "\n",
    "  testCompile \"junit:junit:4.+\"\n",
    "}\n",
    "\n",
    "// Configure 'shadowJar' instead of 'jar' to set up the fat jar.\n",
    "shadowJar {\n",
    "  baseName = '{class_name}' // Name of the fat jar file.\n",
    "  classifier = null       // Set to null, otherwise 'shadow' appends a '-all' to the jar file name.\n",
    "  manifest {\n",
    "    attributes('Main-Class': mainClassName)  // Specify where the main class resides.\n",
    "  }\n",
    "}\n",
    "\"\"\"   \n",
    "    if len(line) == 0:\n",
    "        start = cell.find('class ')\n",
    "        end = cell.find(' {')\n",
    "        class_name = cell[start+6:end]\n",
    "    else:\n",
    "        class_name = line\n",
    "        \n",
    "    \n",
    "    with open('build.gradle', 'w') as f:\n",
    "        f.write(text.replace('{class_name}', class_name))\n",
    "\n",
    "    with open(f'src/main/java/samples/quickstart/{class_name}.java', 'w') as f:\n",
    "        f.write(cell)\n",
    "        \n",
    "    # Build the project.\n",
    "    run(f\"{gradle_path}/bin/gradle --console=plain build\")\n",
    "    run('ls -lh build/libs/')\n",
    "    run('rm outputs/*')\n",
    "    run(f\"{gradle_path}/bin/gradle --console=plain runShadow\")\n",
    "    run('head -n 20 outputs/part*')\n",
    "\n",
    "    print('Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1397c54-3ff7-4188-bc9e-c72bef0d2670",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### A basic Python example of applying a map function to a collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb0dd11-7c70-4a35-bc61-51d36bcfaa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['one', 'two', 'three', 'four']\n",
    "print(list(map(str.title, x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fb1aad-7fe0-4b99-96f4-b30fbaae23ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### To do this in Beam, turn the local collection into a PCollection and apply a Map PTransform on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faedc12b-27d2-4098-85f0-feba2f9e14f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    lines = (\n",
    "        p | beam.Create(['one', 'two', 'three', 'four'])\n",
    "          | beam.Map(str.title)\n",
    "          | beam.Map(print)\n",
    "    )\n",
    "\n",
    "# lines is a PCollection object\n",
    "print('lines = ', lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c8ecc-a678-48eb-9ae8-7fd3e34b8a6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Simple transformation using a lambda instead of a built in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bd9b54f-3a72-4688-a5d1-545077e6b1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One\n",
      "Two\n",
      "Three\n",
      "Four\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    lines = (\n",
    "        p | beam.Create(['one', 'two', 'three', 'four'])\n",
    "          | beam.Map(lambda x : x.title())\n",
    "          | beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977dfeee-d660-47fb-92f7-7a0c3bcbb302",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Simple transformation using a user defined function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bf4cad9-4ed9-4c26-8894-e7d84eff8edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One\n",
      "Two\n",
      "Three\n",
      "Four\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "def title(x):\n",
    "    return x.title()\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    lines = (\n",
    "        p | beam.Create(['one', 'two', 'three', 'four'])\n",
    "          | beam.Map(title)\n",
    "          | beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f09af7-2678-4b0c-879e-c567625a4a83",
   "metadata": {},
   "source": [
    "## The pipe `|` is actually just an operator overload to call the apply method of the pipeline. You would never do this in python, but it helps to understand what is going on under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292e7a3d-99fc-4904-a1cd-baf93d088b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "        lines = ((p | beam.Create(['one', 'two', 'three', 'four']))\n",
    "             .apply(beam.Map(str.title)) \n",
    "             .apply(beam.Map(print)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b057071-63e3-4414-820d-71e07ed0959d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### The Spark equivalent would be to pload a local Python list into a Spark RDD and do a simple transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b110e0-45a0-476b-a2ca-d33cecaec241",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = ( sc.parallelize(['one', 'two', 'three', 'four'])\n",
    "        \n",
    "#           .map(str.title)\n",
    "       )\n",
    "rdd1.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e33b715-0a74-4b46-8736-7798c51b313a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Simple Java transformation using a lambda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420314f-13dd-410f-90a8-569c9f3770ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.transforms.Create;\n",
    "import org.apache.beam.sdk.transforms.MapElements;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import java.util.*;\n",
    "\n",
    "public class Create1 {\n",
    "    public static void main(String[] args) {\n",
    "\n",
    "        String outputsPrefix = \"outputs/part\";\n",
    "        Pipeline p = Pipeline.create();\n",
    "        \n",
    "        PCollection<String> lines = p.apply(Create.of(\"one\", \"two\", \"three\", \"four\"));\n",
    "        lines = lines.apply(MapElements.into(TypeDescriptors.strings()).via((String line) -> line.toUpperCase()));\n",
    "        lines.apply(TextIO.write().to(outputsPrefix));\n",
    "\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3db7b6-c42d-4268-847c-24a9c39df7e8",
   "metadata": {},
   "source": [
    "## Simple transformation using SimpleFunction instead of lambda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ff75092-0480-4784-afba-9616e9e60d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> /opt/gradle-5.0/bin/gradle --console=plain build\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "\u001b[m> Task :compileJava\n",
      "> Task :processResources NO-SOURCE\n",
      "> Task :classes\n",
      "> Task :jar\n",
      "> Task :startScripts\n",
      "> Task :distTar\n",
      "> Task :distZip\n",
      "> Task :shadowJar\n",
      "> Task :startShadowScripts\n",
      "> Task :shadowDistTar\n",
      "> Task :shadowDistZip\n",
      "> Task :assemble\n",
      "> Task :compileTestJava NO-SOURCE\n",
      "> Task :processTestResources NO-SOURCE\n",
      "> Task :testClasses UP-TO-DATE\n",
      "> Task :test NO-SOURCE\n",
      "> Task :check UP-TO-DATE\n",
      "> Task :build\n",
      "\n",
      "BUILD SUCCESSFUL in 17s\n",
      "9 actionable tasks: 9 executed\n",
      "\u001b[m\n",
      ">> ls -lh build/libs/\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "total 127M\n",
      "-rw-r--r-- 1 root root  43M Dec 23 06:38 Create1.jar\n",
      "-rw-r--r-- 1 root root  43M Dec 23 07:02 Create2.jar\n",
      "-rw-r--r-- 1 root root 8.0K Dec 23 07:02 Dataflowclass1.jar\n",
      "-rw-r--r-- 1 root root  43M Dec 23 06:30 WordCount.jar\n",
      "\n",
      ">> rm outputs/*\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "rm: cannot remove 'outputs/*': No such file or directory\n",
      "\n",
      ">> /opt/gradle-5.0/bin/gradle --console=plain runShadow\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "\u001b[m> Task :compileJava UP-TO-DATE\n",
      "> Task :processResources NO-SOURCE\n",
      "> Task :classes UP-TO-DATE\n",
      "> Task :shadowJar UP-TO-DATE\n",
      "> Task :startShadowScripts UP-TO-DATE\n",
      "> Task :installShadowDist\n",
      "\n",
      "> Task :runShadow\n",
      "Dec 23, 2021 7:03:05 AM org.apache.beam.sdk.io.WriteFiles$WriteShardsIntoTempFilesFn processElement\n",
      "INFO: Opening writer e5172a42-6008-491d-8567-7121dfdc6915 for window org.apache.beam.sdk.transforms.windowing.GlobalWindow@4b520ea8 pane PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0} destination null\n",
      "Dec 23, 2021 7:03:05 AM org.apache.beam.sdk.io.WriteFiles$WriteShardsIntoTempFilesFn processElement\n",
      "INFO: Opening writer 3795d635-529a-4bab-b391-ab5d0e77077b for window org.apache.beam.sdk.transforms.windowing.GlobalWindow@4b520ea8 pane PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0} destination null\n",
      "Dec 23, 2021 7:03:05 AM org.apache.beam.sdk.io.WriteFiles$WriteShardsIntoTempFilesFn processElement\n",
      "INFO: Opening writer 690253de-9ec1-44c9-afd0-219a622ea540 for window org.apache.beam.sdk.transforms.windowing.GlobalWindow@4b520ea8 pane PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0} destination null\n",
      "Dec 23, 2021 7:03:05 AM org.apache.beam.sdk.io.FileBasedSink$Writer close\n",
      "INFO: Successfully wrote temporary file /home/jupyter/Dataflowclass1/outputs/.temp-beam-986e4b3d-7d5f-466b-a165-09136850b113/3795d635-529a-4bab-b391-ab5d0e77077b\n",
      "Dec 23, 2021 7:03:05 AM org.apache.beam.sdk.io.FileBasedSink$Writer close\n",
      "INFO: Successfully wrote temporary file /home/jupyter/Dataflowclass1/outputs/.temp-beam-986e4b3d-7d5f-466b-a165-09136850b113/690253de-9ec1-44c9-afd0-219a622ea540\n",
      "Dec 23, 2021 7:03:05 AM org.apache.beam.sdk.io.FileBasedSink$Writer close\n",
      "INFO: Successfully wrote temporary file /home/jupyter/Dataflowclass1/outputs/.temp-beam-986e4b3d-7d5f-466b-a165-09136850b113/e5172a42-6008-491d-8567-7121dfdc6915\n",
      "Dec 23, 2021 7:03:06 AM org.apache.beam.sdk.io.WriteFiles$FinalizeTempFileBundles$FinalizeFn process\n",
      "INFO: Finalizing 3 file results\n",
      "Dec 23, 2021 7:03:06 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation createMissingEmptyShards\n",
      "INFO: Finalizing for destination null num shards 3.\n",
      "Dec 23, 2021 7:03:06 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation createMissingEmptyShards\n",
      "INFO: Creating 1 empty output shards in addition to 3 written for destination null.\n",
      "Dec 23, 2021 7:03:06 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation createMissingEmptyShards\n",
      "INFO: Opening empty writer b440c64c-95c3-4e1a-8d17-dc4d30013d75 for destination null\n",
      "Dec 23, 2021 7:03:06 AM org.apache.beam.sdk.io.FileBasedSink$Writer close\n",
      "INFO: Successfully wrote temporary file /home/jupyter/Dataflowclass1/outputs/.temp-beam-986e4b3d-7d5f-466b-a165-09136850b113/b440c64c-95c3-4e1a-8d17-dc4d30013d75\n",
      "Dec 23, 2021 7:03:06 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation moveToOutputFiles\n",
      "INFO: Will copy temporary file FileResult{tempFilename=/home/jupyter/Dataflowclass1/outputs/.temp-beam-986e4b3d-7d5f-466b-a165-09136850b113/e5172a42-6008-491d-8567-7121dfdc6915, shard=0, window=org.apache.beam.sdk.transforms.windowing.GlobalWindow@4b520ea8, paneInfo=PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0}} to final location /home/jupyter/Dataflowclass1/outputs/part-00000-of-00004\n",
      "Dec 23, 2021 7:03:06 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation moveToOutputFiles\n",
      "INFO: Will copy temporary file FileResult{tempFilename=/home/jupyter/Dataflowclass1/outputs/.temp-beam-986e4b3d-7d5f-466b-a165-09136850b113/3795d635-529a-4bab-b391-ab5d0e77077b, shard=2, window=org.apache.beam.sdk.transforms.windowing.GlobalWindow@4b520ea8, paneInfo=PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0}} to final location /home/jupyter/Dataflowclass1/outputs/part-00002-of-00004\n",
      "Dec 23, 2021 7:03:06 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation moveToOutputFiles\n",
      "INFO: Will copy temporary file FileResult{tempFilename=/home/jupyter/Dataflowclass1/outputs/.temp-beam-986e4b3d-7d5f-466b-a165-09136850b113/690253de-9ec1-44c9-afd0-219a622ea540, shard=3, window=org.apache.beam.sdk.transforms.windowing.GlobalWindow@4b520ea8, paneInfo=PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0}} to final location /home/jupyter/Dataflowclass1/outputs/part-00003-of-00004\n",
      "Dec 23, 2021 7:03:06 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation moveToOutputFiles\n",
      "INFO: Will copy temporary file FileResult{tempFilename=/home/jupyter/Dataflowclass1/outputs/.temp-beam-986e4b3d-7d5f-466b-a165-09136850b113/b440c64c-95c3-4e1a-8d17-dc4d30013d75, shard=1, window=org.apache.beam.sdk.transforms.windowing.GlobalWindow@4b520ea8, paneInfo=PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0}} to final location /home/jupyter/Dataflowclass1/outputs/part-00001-of-00004\n",
      "Dec 23, 2021 7:03:06 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation removeTemporaryFiles\n",
      "WARNING: Failed to match temporary files under: [/home/jupyter/Dataflowclass1/outputs/.temp-beam-986e4b3d-7d5f-466b-a165-09136850b113/].\n",
      "\n",
      "BUILD SUCCESSFUL in 5s\n",
      "5 actionable tasks: 2 executed, 3 up-to-date\n",
      "\u001b[m\n",
      ">> head -n 20 outputs/part*\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "==> outputs/part-00000-of-00004 <==\n",
      "THREE\n",
      "\n",
      "==> outputs/part-00001-of-00004 <==\n",
      "\n",
      "==> outputs/part-00002-of-00004 <==\n",
      "TWO\n",
      "FOUR\n",
      "\n",
      "==> outputs/part-00003-of-00004 <==\n",
      "ONE\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "%%java\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.transforms.Create;\n",
    "import org.apache.beam.sdk.transforms.MapElements;\n",
    "import org.apache.beam.sdk.transforms.SimpleFunction;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import java.util.*;\n",
    "\n",
    "public class Create2 {\n",
    "    public static void main(String[] args) {\n",
    "\n",
    "        String outputsPrefix = \"outputs/part\";\n",
    "        Pipeline p = Pipeline.create();\n",
    "        \n",
    "        PCollection<String> lines = p.apply(\"Create\", Create.of(\"one\", \"two\", \"three\", \"four\"));\n",
    "        lines = lines.apply(\"Uppercase\", MapElements.via(\n",
    "            new SimpleFunction<String, String>() {\n",
    "              @Override\n",
    "              public String apply(String line) {\n",
    "                return line.toUpperCase();\n",
    "              }\n",
    "            }));\n",
    "\n",
    "        lines.apply(\"Write\", TextIO.write().to(outputsPrefix));\n",
    "\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f864aa22-07d5-493c-830c-fa8ec01fca33",
   "metadata": {},
   "source": [
    "## Read from CSV and use Map with lambda.\n",
    "### Also it's a good idea to name the steps so it's easier to debug and monitor them later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53066990-43fc-42c0-9545-ddf723145fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "\n",
    "regionsfilename = 'regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(regionsfilename)\n",
    "          | 'Parse' >> beam.Map(lambda x : x.split(','))\n",
    "          | 'Transform' >> beam.Map(lambda x : (int(x[0]), x[1].upper()))\n",
    "          | 'Write' >> WriteToText(regionsfilename + '.out')\n",
    "#          | 'Print' >> beam.Map(print)\n",
    "    )\n",
    "    #p.run() # implicit in Python when using with block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a83b869e-3b9b-44e9-a786-7ef59882768c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "(1, 'EASTERN')\n",
      "(2, 'WESTERN')\n",
      "(3, 'NORTHERN')\n",
      "(4, 'SOUTHERN')\n"
     ]
    }
   ],
   "source": [
    "! cat regions.csv.out*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28492eea-c43a-43ab-aa7f-ceca3f199a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> /opt/gradle-5.0/bin/gradle --console=plain build\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "\u001b[m> Task :compileJava\n",
      "> Task :processResources NO-SOURCE\n",
      "> Task :classes\n",
      "> Task :jar\n",
      "> Task :startScripts\n",
      "> Task :distTar\n",
      "> Task :distZip\n",
      "> Task :shadowJar\n",
      "> Task :startShadowScripts\n",
      "> Task :shadowDistTar\n",
      "> Task :shadowDistZip\n",
      "> Task :assemble\n",
      "> Task :compileTestJava NO-SOURCE\n",
      "> Task :processTestResources NO-SOURCE\n",
      "> Task :testClasses UP-TO-DATE\n",
      "> Task :test NO-SOURCE\n",
      "> Task :check UP-TO-DATE\n",
      "> Task :build\n",
      "\n",
      "BUILD SUCCESSFUL in 17s\n",
      "9 actionable tasks: 9 executed\n",
      "\u001b[m\n",
      ">> ls -lh build/libs/\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "total 169M\n",
      "-rw-r--r-- 1 root root  43M Dec 23 06:38 Create1.jar\n",
      "-rw-r--r-- 1 root root  43M Dec 23 07:02 Create2.jar\n",
      "-rw-r--r-- 1 root root 8.0K Dec 23 07:03 Dataflowclass1.jar\n",
      "-rw-r--r-- 1 root root  43M Dec 23 07:04 Read1.jar\n",
      "-rw-r--r-- 1 root root  43M Dec 23 06:30 WordCount.jar\n",
      "\n",
      ">> rm outputs/*\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "\n",
      ">> /opt/gradle-5.0/bin/gradle --console=plain runShadow\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "\u001b[m> Task :compileJava UP-TO-DATE\n",
      "> Task :processResources NO-SOURCE\n",
      "> Task :classes UP-TO-DATE\n",
      "> Task :shadowJar UP-TO-DATE\n",
      "> Task :startShadowScripts UP-TO-DATE\n",
      "> Task :installShadowDist\n",
      "\n",
      "> Task :runShadow\n",
      "Dec 23, 2021 7:04:15 AM org.apache.beam.sdk.io.FileBasedSource getEstimatedSizeBytes\n",
      "INFO: Filepattern regions.csv matched 1 files with total size 42\n",
      "Dec 23, 2021 7:04:15 AM org.apache.beam.sdk.io.FileBasedSource split\n",
      "INFO: Splitting filepattern regions.csv into bundles of size 10 took 1 ms and produced 1 files and 5 bundles\n",
      "Dec 23, 2021 7:04:16 AM org.apache.beam.sdk.io.WriteFiles$WriteShardsIntoTempFilesFn processElement\n",
      "INFO: Opening writer 48634d21-dee8-43dd-b41f-d4444dc1bc05 for window org.apache.beam.sdk.transforms.windowing.GlobalWindow@210ab13f pane PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0} destination null\n",
      "Dec 23, 2021 7:04:16 AM org.apache.beam.sdk.io.WriteFiles$WriteShardsIntoTempFilesFn processElement\n",
      "INFO: Opening writer 265be4d7-7549-4f0a-98cf-736fc1fb0bdb for window org.apache.beam.sdk.transforms.windowing.GlobalWindow@210ab13f pane PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0} destination null\n",
      "Dec 23, 2021 7:04:16 AM org.apache.beam.sdk.io.FileBasedSink$Writer close\n",
      "INFO: Successfully wrote temporary file /home/jupyter/Dataflowclass1/outputs/.temp-beam-f2af6169-2fbb-4464-b14e-25c3f44c1aad/48634d21-dee8-43dd-b41f-d4444dc1bc05\n",
      "Dec 23, 2021 7:04:16 AM org.apache.beam.sdk.io.FileBasedSink$Writer close\n",
      "INFO: Successfully wrote temporary file /home/jupyter/Dataflowclass1/outputs/.temp-beam-f2af6169-2fbb-4464-b14e-25c3f44c1aad/265be4d7-7549-4f0a-98cf-736fc1fb0bdb\n",
      "Dec 23, 2021 7:04:16 AM org.apache.beam.sdk.io.WriteFiles$FinalizeTempFileBundles$FinalizeFn process\n",
      "INFO: Finalizing 2 file results\n",
      "Dec 23, 2021 7:04:16 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation createMissingEmptyShards\n",
      "INFO: Finalizing for destination null num shards 2.\n",
      "Dec 23, 2021 7:04:16 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation createMissingEmptyShards\n",
      "INFO: Creating 1 empty output shards in addition to 2 written for destination null.\n",
      "Dec 23, 2021 7:04:16 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation createMissingEmptyShards\n",
      "INFO: Opening empty writer 49785c25-8338-45ae-bc13-5319b102aef5 for destination null\n",
      "Dec 23, 2021 7:04:16 AM org.apache.beam.sdk.io.FileBasedSink$Writer close\n",
      "INFO: Successfully wrote temporary file /home/jupyter/Dataflowclass1/outputs/.temp-beam-f2af6169-2fbb-4464-b14e-25c3f44c1aad/49785c25-8338-45ae-bc13-5319b102aef5\n",
      "Dec 23, 2021 7:04:16 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation moveToOutputFiles\n",
      "INFO: Will copy temporary file FileResult{tempFilename=/home/jupyter/Dataflowclass1/outputs/.temp-beam-f2af6169-2fbb-4464-b14e-25c3f44c1aad/48634d21-dee8-43dd-b41f-d4444dc1bc05, shard=2, window=org.apache.beam.sdk.transforms.windowing.GlobalWindow@210ab13f, paneInfo=PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0}} to final location /home/jupyter/Dataflowclass1/outputs/part-00002-of-00003\n",
      "Dec 23, 2021 7:04:16 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation moveToOutputFiles\n",
      "INFO: Will copy temporary file FileResult{tempFilename=/home/jupyter/Dataflowclass1/outputs/.temp-beam-f2af6169-2fbb-4464-b14e-25c3f44c1aad/265be4d7-7549-4f0a-98cf-736fc1fb0bdb, shard=0, window=org.apache.beam.sdk.transforms.windowing.GlobalWindow@210ab13f, paneInfo=PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0}} to final location /home/jupyter/Dataflowclass1/outputs/part-00000-of-00003\n",
      "Dec 23, 2021 7:04:16 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation moveToOutputFiles\n",
      "INFO: Will copy temporary file FileResult{tempFilename=/home/jupyter/Dataflowclass1/outputs/.temp-beam-f2af6169-2fbb-4464-b14e-25c3f44c1aad/49785c25-8338-45ae-bc13-5319b102aef5, shard=1, window=org.apache.beam.sdk.transforms.windowing.GlobalWindow@210ab13f, paneInfo=PaneInfo{isFirst=true, isLast=true, timing=ON_TIME, index=0, onTimeIndex=0}} to final location /home/jupyter/Dataflowclass1/outputs/part-00001-of-00003\n",
      "Dec 23, 2021 7:04:16 AM org.apache.beam.sdk.io.FileBasedSink$WriteOperation removeTemporaryFiles\n",
      "WARNING: Failed to match temporary files under: [/home/jupyter/Dataflowclass1/outputs/.temp-beam-f2af6169-2fbb-4464-b14e-25c3f44c1aad/].\n",
      "\n",
      "BUILD SUCCESSFUL in 5s\n",
      "5 actionable tasks: 2 executed, 3 up-to-date\n",
      "\u001b[m\n",
      ">> head -n 20 outputs/part*\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "==> outputs/part-00000-of-00003 <==\n",
      "3,NORTHERN\n",
      "4,SOUTHERN\n",
      "1,EASTERN\n",
      "\n",
      "==> outputs/part-00001-of-00003 <==\n",
      "\n",
      "==> outputs/part-00002-of-00003 <==\n",
      "2,WESTERN\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "%%java\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.transforms.MapElements;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "\n",
    "public class Read1 {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String regionsInputFileName = \"regions.csv\";\n",
    "        String outputsPrefix = \"outputs/part\";\n",
    "\n",
    "        PCollection<String> regions = p\n",
    "            .apply(\"Read\", TextIO.read().from(regionsInputFileName))\n",
    "            .apply(\"Parse\", MapElements.into(TypeDescriptors.strings()).via((String element) -> element.toUpperCase()));\n",
    "        \n",
    "        regions.apply(TextIO.write().to(outputsPrefix));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee88479c-60c3-48c3-b8c1-3e8113f0d39a",
   "metadata": {},
   "source": [
    "## Read from CSV and use ParDo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94887a8a-f945-414f-bfaa-2e72fa1aa746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Eastern')\n",
      "(2, 'Western')\n",
      "(3, 'Northern')\n",
      "(4, 'Southern')\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "\n",
    "class RegionParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        #return [(int(regionid), regionname)] # ParDo's need to return a list\n",
    "        yield (int(regionid), regionname) # Can also use yield instead of returning a list\n",
    "#        yield (int(regionid), regionname.upper()) # Include a transformation instead of doing it as a separate step\n",
    "\n",
    "regionsfilename = 'regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(regionsfilename)\n",
    "          | 'Parse' >> beam.ParDo(RegionParseTuple())\n",
    "          #| 'Write' >> WriteToText('regions.out')\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14327a97-f8cd-489b-87eb-82ad962f701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> /opt/gradle-5.0/bin/gradle --console=plain build\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "\u001b[m\n",
      "> Task :compileJava FAILED\n",
      "/home/jupyter/Dataflowclass1/src/main/java/samples/quickstart/Read1.java:21: error: incompatible types: inferred type does not conform to equality constraint(s)\n",
      "            .apply(\"Parse\", ParDo.of(new DoFn<String, String[]>() {\n",
      "                  ^\n",
      "    inferred: String\n",
      "    equality constraints(s): String,String[]\n",
      "1 error\n",
      "\n",
      "FAILURE: Build failed with an exception.\n",
      "\n",
      "* What went wrong:\n",
      "Execution failed for task ':compileJava'.\n",
      "> Compilation failed; see the compiler error output for details.\n",
      "\n",
      "* Try:\n",
      "Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.\n",
      "\n",
      "* Get more help at https://help.gradle.org\n",
      "\n",
      "BUILD FAILED in 0s\n",
      "1 actionable task: 1 executed\n",
      "\u001b[m\n",
      ">> ls -lh build/libs/\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "total 169M\n",
      "-rw-r--r-- 1 root root  43M Dec 23 06:38 Create1.jar\n",
      "-rw-r--r-- 1 root root  43M Dec 23 07:02 Create2.jar\n",
      "-rw-r--r-- 1 root root 8.0K Dec 23 07:03 Dataflowclass1.jar\n",
      "-rw-r--r-- 1 root root  43M Dec 23 07:04 Read1.jar\n",
      "-rw-r--r-- 1 root root  43M Dec 23 06:30 WordCount.jar\n",
      "\n",
      ">> rm outputs/*\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "rm: cannot remove 'outputs/*': No such file or directory\n",
      "\n",
      ">> /opt/gradle-5.0/bin/gradle --console=plain runShadow\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "\u001b[m\n",
      "> Task :compileJava FAILED\n",
      "/home/jupyter/Dataflowclass1/src/main/java/samples/quickstart/Read1.java:21: error: incompatible types: inferred type does not conform to equality constraint(s)\n",
      "            .apply(\"Parse\", ParDo.of(new DoFn<String, String[]>() {\n",
      "                  ^\n",
      "    inferred: String\n",
      "    equality constraints(s): String,String[]\n",
      "1 error\n",
      "\n",
      "FAILURE: Build failed with an exception.\n",
      "\n",
      "* What went wrong:\n",
      "Execution failed for task ':compileJava'.\n",
      "> Compilation failed; see the compiler error output for details.\n",
      "\n",
      "* Try:\n",
      "Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.\n",
      "\n",
      "* Get more help at https://help.gradle.org\n",
      "\n",
      "BUILD FAILED in 0s\n",
      "1 actionable task: 1 executed\n",
      "\u001b[m\n",
      ">> head -n 20 outputs/part*\n",
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "head: cannot open 'outputs/part*' for reading: No such file or directory\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "%%java\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "\n",
    "public class Read1 {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String regionsInputFileName = \"regions.csv\";\n",
    "        String outputsPrefix = \"outputs/part\";\n",
    "\n",
    "\n",
    "        PCollection<String> regions = p\n",
    "            .apply(\"Read\", TextIO.read().from(regionsInputFileName))\n",
    "            .apply(\"Parse\", ParDo.of(new DoFn<String, String[]>() {\n",
    "                public void processElement(ProcessContext c) {\n",
    "                    String element = c.element();\n",
    "                    String[] elements = element.split(\",\");\n",
    "                    c.output(elements);\n",
    "                }\n",
    "            }));\n",
    "        \n",
    "        regions.apply(TextIO.write().to(outputsPrefix));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26457094-4a48-4a80-8aa0-876ca3107664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01. Apache Beam 2.34.0 for Python 3",
   "language": "python",
   "name": "01-apache-beam-2.34.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
