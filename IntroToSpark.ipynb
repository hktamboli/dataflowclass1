{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRjOMwVYiaET"
   },
   "source": [
    "### Install a Spark docker using the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AQQ_mVrQiaEW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "latest: Pulling from bitnami/spark\n",
      "Digest: sha256:45acd47b917751eb2f21b03cf4f21b567e44db7c78b2774f06c947caf5d3a9a6\n",
      "Status: Image is up to date for bitnami/spark:latest\n",
      "docker.io/bitnami/spark:latest\n",
      "Error response from daemon: network with name spark_network already exists\n"
     ]
    }
   ],
   "source": [
    "! docker pull bitnami/spark && \\\n",
    "docker network create spark_network && \\\n",
    "docker run -d --name spark --network=spark_network -e SPARK_MODE=master bitnami/spark\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stLlpheLWf_j"
   },
   "source": [
    "### Create the Spark context to start a session and connect to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing pyspark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/23 02:39:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark initialized\n",
      "<SparkContext master=local[*] appName=Notebook> <pyspark.sql.session.SparkSession object at 0x7faeefd1fed0>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def initspark(appname = \"Notebook\", servername = \"local[*]\"):\n",
    "    print ('initializing pyspark')\n",
    "    conf = SparkConf().setAppName(appname).setMaster(servername)\n",
    "    sc = SparkContext(conf=conf)\n",
    "    spark = SparkSession.builder.appName(appname).enableHiveSupport().getOrCreate()\n",
    "    sc.setLogLevel(\"WARN\")\n",
    "    print ('pyspark initialized')\n",
    "    return sc, spark, conf\n",
    "\n",
    "sc, spark, conf = initspark()\n",
    "print(sc, spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push a local Python list into a Spark RDD and do a simple transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oC4ujoSFWf_m"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One', 'Two', 'Three', 'Four']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = ( sc.parallelize(['one', 'two', 'three', 'four'])\n",
    "           .map(str.title)\n",
    "       )\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same code but with a lambda in case you don't have a built in function already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One', 'Two', 'Three', 'Four']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = ( sc.parallelize(['one', 'two', 'three', 'four'])\n",
    "           .map(lambda x : x.title())\n",
    "       )\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uH-N0vuoWf_v"
   },
   "source": [
    "### Read a text file from the local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6iEi36CWf_w"
   },
   "outputs": [],
   "source": [
    "shake = sc.textFile('/class/datasets/text/shakespeare.txt')\n",
    "print(shake.count())\n",
    "print(shake.take(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdStyssVWgAU"
   },
   "source": [
    "### Parallelize will load manually created data into the spark cluster into an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XN8v44eNWgAW"
   },
   "outputs": [],
   "source": [
    "r = sc.parallelize(range(1,11))\n",
    "print(r.collect())\n",
    "print(r.take(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQVoJi3CWgAo"
   },
   "source": [
    "### Load a folder stored on HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tL89iGyOWgAq"
   },
   "outputs": [],
   "source": [
    "cat = sc.textFile('hdfs://localhost:9000/categories')\n",
    "print(cat.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlQ_B5enWgAz"
   },
   "source": [
    "### Try some different actions to fetch data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCqmMuFLWgA1"
   },
   "outputs": [],
   "source": [
    "print(cat.takeOrdered(5))\n",
    "print(cat.top(5))\n",
    "print(cat.takeSample(False,5))\n",
    "cat.foreach(lambda x : print(x.upper)) # does not display properly in notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqD-2-05WgBC"
   },
   "source": [
    "### Save the results in an RDD to disk. Note how it makes a folder and fills it with as many files as there are nodes solving the problem. Also, you must make sure that the folder does not exist or it throws an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "au3L8fPDWgBE"
   },
   "outputs": [],
   "source": [
    "! rm -r /class/file1.txt\n",
    "cat.saveAsTextFile('hdfs://localhost:9000/file1.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wA9xkJu41Pd2"
   },
   "outputs": [],
   "source": [
    "! hadoop fs -ls /file1.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bt-1Pg9DWf_1"
   },
   "source": [
    "### Use the map method to apply a function call on each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3wwsvU6Wf_2"
   },
   "outputs": [],
   "source": [
    "shake2 = shake.map(str.upper)\n",
    "shake2.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qx0qMtPyWf_9"
   },
   "source": [
    "### Using the split method you get a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Szr74Z3AWf__"
   },
   "outputs": [],
   "source": [
    "shake3 = shake.map(lambda x : x.split(' '))\n",
    "shake3.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-L0L9BiWgAF"
   },
   "outputs": [],
   "source": [
    "### The flatMap method flattens the inner list to return one big list of strings instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gP2oWr9CWgAI"
   },
   "outputs": [],
   "source": [
    "shake4 = shake.flatMap(lambda x : x.split(' '))\n",
    "shake4.take(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TK88KrBTWgBN"
   },
   "outputs": [],
   "source": [
    "print(cat.map(str.upper).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vn43ZGHWgBT"
   },
   "source": [
    "### Parse the string into a tuple to resemble a record structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJ2bIehvWgBU"
   },
   "outputs": [],
   "source": [
    "cat1 = cat.map(lambda x : tuple(x.split(',')))\n",
    "cat1 = cat1.map(lambda x : (int(x[0]), x[1], x[2]))\n",
    "cat1.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ko12EKvYWgBX"
   },
   "source": [
    "## LAB: ## \n",
    "### Put the regions folder found in /class/datasets/northwind/CSV/regions into HDFS. Read it into an RDD and convert it into a tuple shape.\n",
    "<br>\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>\n",
    "Use hadoop fs -put or hdfs dfs -put\n",
    "<br>\n",
    "Read the file using sc.textFile\n",
    "<br>\n",
    "Do a map to split and another to convert the datatypes\n",
    "<br>\n",
    "<br>\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "! hadoop fs -put /class/datasets/northwind/CSV/regions /regions\n",
    "regions = sc.textFile('hdfs://localhost:9000/regions')\n",
    "regions = regions.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1]))\n",
    "print(regions.collect())\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZWX8p7nWgBY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lc8uQPq1WgBi"
   },
   "source": [
    "### You can chain multiple transformations together to do it all in one step.\n",
    "#### Here we converted the datatypes to int, then turned the tuple into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6nLPu-glWgBj"
   },
   "outputs": [],
   "source": [
    "cat2 = cat.map(lambda x : tuple(x.split(','))) \\\n",
    "      .map(lambda x : (int(x[0]), x[1], x[2])) \\\n",
    "      .map(lambda x : dict(zip(['CategoryID', 'Name', 'Description'], x)))\n",
    "cat2.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1451bwFaWgBn"
   },
   "source": [
    "### The filter method takes a lambda that returns a True or False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQzyEYCYWgBo"
   },
   "outputs": [],
   "source": [
    "cat2.filter(lambda x : x['CategoryID'] <= 5).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22q9mGVrWgBu"
   },
   "source": [
    "### The filter expressions can be more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gIEc83jgWgBw"
   },
   "outputs": [],
   "source": [
    "cat2.filter(lambda x : x['CategoryID'] % 2 == 0 and 'e' in x['Name']).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-mS0TT9WgB6"
   },
   "source": [
    "### The sortBy method returns an expression that is used to sort the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQwZ4Z_vWgB7"
   },
   "outputs": [],
   "source": [
    "cat2.sortBy(lambda x : x['Description']).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4gTRsTrWgCD"
   },
   "source": [
    "### sortBy has an option ascending parameter to sort in reverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRq5r1UAWgCF"
   },
   "outputs": [],
   "source": [
    "cat1.sortBy(lambda x : x[0], ascending = False).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r40EeKuEWgCK"
   },
   "source": [
    "## LAB:##\n",
    "### Try to sort region in descending order by ID and then by name in ascending order. ###\n",
    "\n",
    "<br>\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>\n",
    "Use sortByKey and sortBy respectively\n",
    "<br>\n",
    "sortBy needs a lambda\n",
    "<br><br>\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "print(regions.sortByKey(ascending = False).collect())\n",
    "print(regions.sortBy(lambda x : x[1]).collect())\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eF5E8yZyWgCL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nbV0tx11Pe_"
   },
   "source": [
    "### The following are more complex examples of using Spark to do things like JOIN and GROUP BY. For the most part these methods are replaced by the newer DataFrame methods which we will explore in the next section. We will skip a detailed explanation of the following but leave it in for self study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4e5XbSMWgCO"
   },
   "source": [
    "### Reshape categories from a tuple of three elements like (1, 'Beverages', 'Soft drinks') to a tuple with two elements (key, value) like (1, ('Beverages', 'Soft drinks'))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yu2izny5WgCP"
   },
   "outputs": [],
   "source": [
    "cat3 = cat1.map(lambda x : (x[0], (x[1], x[2])))\n",
    "cat3.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ge3v9g1NWgCR"
   },
   "source": [
    "### The sortByKey method does not require a function as a parameter if the data is structured into a tuple of the shape (key, value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MF6jtxiKWgCS"
   },
   "outputs": [],
   "source": [
    "cat3.sortByKey(ascending=False).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ubk9dpCWWgCX"
   },
   "source": [
    "### Read in another CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dprx0Q-_WgCY"
   },
   "outputs": [],
   "source": [
    "prod = shake = sc.textFile('/class/datasets/northwind/CSV/products')\n",
    "print(prod.count())\n",
    "prod.take(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtLTEKPgWgCb"
   },
   "source": [
    "### Split it up and just keep the ProductID, ProductName, CategoryID, Price, Quantity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BAEwAdJUWgCc"
   },
   "outputs": [],
   "source": [
    "prod1 = prod.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1], int(x[3]), float(x[5]), int(x[6])))\n",
    "prod1.take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "962IynkWWgCg"
   },
   "source": [
    "### Reshape it to a key value tuple where category is the key and the other fields are the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EIP64lugWgCh"
   },
   "outputs": [],
   "source": [
    "prod2 = prod1.map(lambda x : (x[2], (x[0], x[1], x[3], x[4])))\n",
    "prod2.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1hNliYJWgCl"
   },
   "outputs": [],
   "source": [
    "cat3.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aS2NAAXHWgCo"
   },
   "source": [
    "### Both c3 and prod2 are in key value tuple format so they can be joined to produce a new tuple of (key, (cat, prod))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZmbDZCGWgCp"
   },
   "outputs": [],
   "source": [
    "joined = cat3.join(prod2)\n",
    "joined.sortByKey().take(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OI8VffUbWgCt"
   },
   "source": [
    "## LAB: ##\n",
    "### Load territories into HDFS and join it to regions. ###\n",
    "\n",
    "\n",
    "<br>\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>\n",
    "Put /class/datasets/northwind/CSV/territories into HDFS\n",
    "<br>\n",
    "Use sc.textFile to read it into an RDD\n",
    "<br>\n",
    "Use map to split and convert it to the proper datatypes\n",
    "<br>\n",
    "Use the join method\n",
    "<br><br>\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "! hadoop fs -put /class/datasets/northwind/CSV/territories /\n",
    "\n",
    "territories = sc.textFile('hdfs://localhost:9000/territories')\n",
    "territories = territories.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1], int(x[2])))\n",
    "print(territories.collect())\n",
    "\n",
    "region_territories = regions.join(territories.map(lambda x : (x[2], (x[0],x[1]))))\n",
    "print(region_territories.collect())\n",
    "# Reshape it to make it look more normal. The * in front of the x is a python unpacking trick\n",
    "region_territories = region_territories.map(lambda x : (x[0], (x[1][0], *x[1][1])))\n",
    "print(region_territories.collect())\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jG-jz9oCWgCu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7_N8D4vWgCy"
   },
   "source": [
    "### The groupBy methods are seldom used but they can produce hierarchies where children records are embedded inside a parent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8XG_Zh1WgC5"
   },
   "outputs": [],
   "source": [
    "group1 = prod2.groupByKey()\n",
    "group1.take(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlfoIOfvWgCz"
   },
   "outputs": [],
   "source": [
    "list(group1.take(1)[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mi73ZFk4WgC_"
   },
   "outputs": [],
   "source": [
    "group2 = [(key, list(it)) for key, it in group1.collect()]\n",
    "for k,v in group2:\n",
    "    print ('Key:', k)\n",
    "    for x in v:\n",
    "        print(x)\n",
    "#print (group2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiQFuV5bWgDF"
   },
   "source": [
    "### The reduce methods take a function as a parameter that tells Spark how to accumulate the values for each group. The function takes two parameters; the first is the accumulated value and the second is the next value in the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FCNryN_WgDG"
   },
   "outputs": [],
   "source": [
    "shake4.map(lambda x : (x, 1)).reduceByKey(lambda x, y : x + y).sortBy(lambda x : x[1], ascending = False).take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agTxvAFjWgDK"
   },
   "source": [
    "## LAB: ## \n",
    "### Use the territories RDD to count how many territories are in each region. \n",
    "### Display the results in regionID order and then descending order based on the counts.\n",
    "<br>\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>\n",
    "Use map to put the key first then reduceByKey to accumulate the values\n",
    "<br>\n",
    "Use sortByKey to sort by regionID and sortBy with a lambda to sort by counts\n",
    "<br><br>\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "region_count = territories.map(lambda x : (x[2], 1)).reduceByKey(lambda x, y: x + y)\n",
    "print(region_count.sortByKey().collect())\n",
    "print(region_count.sortBy(lambda x : x[1], ascending = False).collect())\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PF1xAAZVWgDL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GejVQTZMWgDP"
   },
   "source": [
    "### In this example, we are adding up all the prices for each categoryID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTQCEouBWgDQ"
   },
   "outputs": [],
   "source": [
    "red1 = prod2.map(lambda x : (x[0], x[1][2])).reduceByKey(lambda x, y: x + y)\n",
    "red1.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH_hY2_PWgDV"
   },
   "source": [
    "### To accumulate more than one value, use a tuple to hold as many values as you want to aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtQX34KiWgDW"
   },
   "outputs": [],
   "source": [
    "red1 = prod2.map(lambda x : (x[0], (x[1][2], x[1][3], 1))).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2]))\n",
    "red1.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-e6do4WZWgDg"
   },
   "source": [
    "### Some Python magic can make things easier in the long run.\n",
    "Named tuples make accessing the elements of the row easier.\n",
    "Unpacking using the * is a neat Python trick that is widely used.\n",
    " \n",
    "datetime has function to convert a string into a date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoiIjU2wWgDl"
   },
   "outputs": [],
   "source": [
    "mort = sc.textFile('/class/datasets/finance/30YearMortgage.csv')\n",
    "head = mort.first()\n",
    "mort = mort.filter(lambda x : x != head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fyvY687WgDp"
   },
   "outputs": [],
   "source": [
    "from datetime import date, datetime\n",
    "from collections import namedtuple\n",
    "Rate = namedtuple('Rate','date fed_fund_rate avg_rate_30year')\n",
    "mort1 = mort.map(lambda x : Rate(*(x.split(','))))\n",
    "mort2 = mort1.map(lambda x : Rate(datetime.strptime(x.date, '%Y-%m').date(), float(x.fed_fund_rate), float(x.avg_rate_30year)))\n",
    "mort2.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4i7WOKwIWgDt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mort2.filter(lambda x : x.fed_fund_rate > .1 ).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjXdMjdxWgDw"
   },
   "source": [
    "### HOMEWORK:\n",
    "1. The creditcard.csv dataset provides sample data on credit card transactions.\n",
    "2. Load the file into HDFS.\n",
    "3. Load the file into an RDD.\n",
    "4. Parse the file into a tuple or namedtuple or dictionary.\n",
    "5. Make sure to convert columns to the right data types.\n",
    "6. You can ignore any columns you don’t need for the solution.\n",
    "7. Filter the data to show only transactions made by women.\n",
    "8. Calculate the amount spent in each city.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQNAL1_xWgDx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Day1-IntroToSpark.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
