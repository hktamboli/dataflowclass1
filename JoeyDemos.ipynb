{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31a2bba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        var import_html = () => {\n",
       "          ['https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html'].forEach(href => {\n",
       "            var link = document.createElement('link');\n",
       "            link.rel = 'import'\n",
       "            link.href = href;\n",
       "            document.head.appendChild(link);\n",
       "          });\n",
       "        }\n",
       "        if ('import' in document.createElement('link')) {\n",
       "          import_html();\n",
       "        } else {\n",
       "          var webcomponentScript = document.createElement('script');\n",
       "          webcomponentScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js';\n",
       "          webcomponentScript.type = 'text/javascript';\n",
       "          webcomponentScript.onload = function(){\n",
       "            import_html();\n",
       "          };\n",
       "          document.head.appendChild(webcomponentScript);\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONE\n",
      "TWO\n",
      "THREE\n",
      "FOUR\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    lines = (\n",
    "        p | 'Create' >> beam.Create(['one', 'two', 'three', 'four'])\n",
    "          | 'Uppercase' >> beam.Map(str.upper)\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f956b571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JoeyDemos.ipynb       leftjoin.py    simple3-dataflow.sh  territories.py\n",
      "README.md\t      products.json  simple3-local.sh\t  wordcount-dataflow.sh\n",
      "aggregate1.py\t      regions.csv    simple3.py\t\t  wordcount.py\n",
      "aggregate2.py\t      simple1.py     simple3_custom.py\n",
      "dataflow_template.py  simple2.py     territories.avro\n"
     ]
    }
   ],
   "source": [
    "#! gsutil cp gs://qwiklabs-gcp-02-7b5be618aa6e/* .\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae30ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 'EASTERN')\n",
      "(20, 'WESTERN')\n",
      "(30, 'NORTHERN')\n",
      "(40, 'SOUTHERN')\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "\n",
    "filename = 'regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(filename)\n",
    "          | 'Split' >> beam.Map(lambda x : tuple(x.split(',')))\n",
    "          | 'Transform' >> beam.Map(lambda x : (int(x[0]) * 10, x[1].upper()))\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15267f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "class RegionSplit(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        #return [(regionid, regionname)] # ParDo's need to return a list\n",
    "        yield (regionid, regionname) # Can also use yield instead of returning a list\n",
    "\n",
    "# using a ParDo and DoFn instead of a Map\n",
    "filename = 'regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    lines = p | 'Read' >> ReadFromText(filename)\n",
    "    records = lines | 'Split' >> beam.ParDo(RegionSplit())\n",
    "    records | 'Write' >> WriteToText('regions.out')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bee81f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', 'Eastern')\n",
      "('2', 'Western')\n",
      "('3', 'Northern')\n",
      "('4', 'Southern')\n"
     ]
    }
   ],
   "source": [
    "! cat regions.out*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ae5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A template to import the default package and parse the arguments\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import re\n",
    "\n",
    "#from past.builtins import unicode\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "class RegionSplit(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        #return [(regionid, regionname)] # ParDo's need to return a list\n",
    "        yield (regionid, regionname) # Can also use yield instead of returning a list\n",
    "\n",
    "def run(argv=None, save_main_session=True):\n",
    "  \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--input',\n",
    "      dest='input',\n",
    "      default='gs://dataflowclass1-bucket/regions.csv',\n",
    "      help='Input file to process.')\n",
    "  parser.add_argument(\n",
    "      '--output',\n",
    "      dest='output',\n",
    "      default = 'gs://dataflowclass1-bucket/regions_output',      \n",
    "      help='Output file to write results to.')\n",
    "  known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "\n",
    "  # We use the save_main_session option because one or more DoFn's in this\n",
    "  # workflow rely on global context (e.g., a module imported at module level).\n",
    "  pipeline_options = PipelineOptions(pipeline_args)\n",
    "  pipeline_options.view_as(SetupOptions).save_main_session = save_main_session\n",
    "\n",
    "  # The pipeline will be run on exiting the with block.\n",
    "  with beam.Pipeline(options=pipeline_options) as p:\n",
    "    lines = p | 'Read' >> ReadFromText(known_args.input)\n",
    "    records = lines | 'Split' >> beam.ParDo(RegionSplit())\n",
    "    uppercase = records | 'Uppercase' >> beam.Map(lambda x : (int(x[0]), x[1].upper()))\n",
    "    uppercase | 'Write' >> WriteToText(known_args.output)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  logging.getLogger().setLevel(logging.INFO)\n",
    "  run()\n",
    "\n",
    "\n",
    "# using a ParDo and DoFn instead of a Map\n",
    "filename = 'regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    lines = p | 'Read' >> ReadFromText(filename)\n",
    "    records = lines | 'Split' >> beam.ParDo(RegionSplit())\n",
    "    records | 'Write' >> WriteToText('regions2.out')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "394f91c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evens\n",
      "Odds\n",
      "(1, 'Eastern', 'Odd')\n",
      "(2, 'Western', 'Even')\n",
      "(3, 'Northern', 'Odd')\n",
      "(4, 'Southern', 'Even')\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam import pvalue\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "\n",
    "class OddEvenRegionSplit(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        if int(regionid) % 2 == 0:\n",
    "            yield pvalue.TaggedOutput('Even', (int(regionid), regionname, 'Even'))\n",
    "        else:\n",
    "            yield pvalue.TaggedOutput('Odd', (int(regionid), regionname, 'Odd'))\n",
    "\n",
    "# using a ParDo and DoFn instead of a Map\n",
    "filename = 'regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    lines = p | 'Read' >> ReadFromText(filename)\n",
    "    evens, odds = lines | 'Split' >> beam.ParDo(OddEvenRegionSplit()).with_outputs(\"Even\", \"Odd\")\n",
    "    \n",
    "    print('Evens')\n",
    "    evens | 'Print Evens' >> beam.Map(print)\n",
    "\n",
    "    print('Odds')\n",
    "    odds | 'Print Odds' >> beam.Map(print)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d616af9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Beam 2.25.0 for Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
