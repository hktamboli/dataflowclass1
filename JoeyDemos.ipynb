{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b37c513",
   "metadata": {},
   "source": [
    "## Simple transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df5ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    lines = (\n",
    "        p | 'Create' >> beam.Create(['one', 'two', 'three', 'four'])\n",
    "          | 'Uppercase' >> beam.Map(str.upper)\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d92aedf",
   "metadata": {},
   "source": [
    "## The pipe | is actually just an operator overload to call the apply method of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a6569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "        lines = p | 'Create' >> beam.Create(['one', 'two', 'three', 'four'])\n",
    "        lines2 = (\n",
    "            p.apply(beam.Map(str.title), lines, 'titlecase')\n",
    "             .apply(beam.Map(print))\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ebe2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956c2553",
   "metadata": {},
   "source": [
    "## Read from CSV and use Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb833646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "\n",
    "regionsfilename = 'regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(regionsfilename)\n",
    "          | 'Split' >> beam.Map(lambda x : tuple(x.split(',')))\n",
    "          | 'Transform' >> beam.Map(lambda x : (int(x[0]) * 10, x[1].upper()))\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e290b78",
   "metadata": {},
   "source": [
    "## Read from CSV and use ParDo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae84735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "class RegionSplit(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        #return [(int(regionid), regionname)] # ParDo's need to return a list\n",
    "        yield (int(regionid), regionname) # Can also use yield instead of returning a list\n",
    "\n",
    "regionsfilename = 'regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(regionsfilename)\n",
    "          | 'Split' >> beam.ParDo(RegionSplit())\n",
    "          | 'Write' >> WriteToText('regions.out')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b276b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat regions.out*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa687f01",
   "metadata": {},
   "source": [
    "## Template showing a full program that can read the command line args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A template to import the default package and parse the arguments\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import re\n",
    "\n",
    "#from past.builtins import unicode\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "class RegionSplit(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        #return [(regionid, regionname)] # ParDo's need to return a list\n",
    "        yield (int(regionid), regionname) # Can also use yield instead of returning a list\n",
    "\n",
    "def run(argv=None, save_main_session=True):\n",
    "  \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--input',\n",
    "      dest='input',\n",
    "      default='gs://dataflowclass1-bucket/regions.csv',\n",
    "      help='Input file to process.')\n",
    "  parser.add_argument(\n",
    "      '--output',\n",
    "      dest='output',\n",
    "      default = 'gs://dataflowclass1-bucket/regions_output',      \n",
    "      help='Output file to write results to.')\n",
    "  known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "\n",
    "  # We use the save_main_session option because one or more DoFn's in this\n",
    "  # workflow rely on global context (e.g., a module imported at module level).\n",
    "  pipeline_options = PipelineOptions(pipeline_args)\n",
    "  pipeline_options.view_as(SetupOptions).save_main_session = save_main_session\n",
    "\n",
    "  # The pipeline will be run on exiting the with block.\n",
    "  with beam.Pipeline(options=pipeline_options) as p:\n",
    "    lines = p | 'Read' >> ReadFromText(known_args.input)\n",
    "    records = lines | 'Split' >> beam.ParDo(RegionSplit())\n",
    "    uppercase = records | 'Uppercase' >> beam.Map(lambda x : (int(x[0]), x[1].upper()))\n",
    "    uppercase | 'Write' >> WriteToText(known_args.output)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  logging.getLogger().setLevel(logging.INFO)\n",
    "  run()\n",
    "\n",
    "\n",
    "filename = 'regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    lines = p | 'Read' >> ReadFromText(filename)\n",
    "    records = lines | 'Split' >> beam.ParDo(RegionSplit())\n",
    "    records | 'Write' >> WriteToText('regions2.out')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b5deb7",
   "metadata": {},
   "source": [
    "## Example of how to create a split ParDo with multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dc492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam import pvalue\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "\n",
    "class OddEvenRegionSplit(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        if int(regionid) % 2 == 0:\n",
    "            yield pvalue.TaggedOutput('Even', (int(regionid), regionname, 'Even'))\n",
    "        else:\n",
    "            yield pvalue.TaggedOutput('Odd', (int(regionid), regionname, 'Odd'))\n",
    "\n",
    "regionsfilename = 'regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    lines = p | 'Read' >> ReadFromText(regionsfilename)\n",
    "    evens, odds = lines | 'Split' >> beam.ParDo(OddEvenRegionSplit()).with_outputs(\"Even\", \"Odd\")\n",
    "    \n",
    "    print('Evens')\n",
    "    evens | 'Print Evens' >> beam.Map(print)\n",
    "\n",
    "    print('Odds')\n",
    "    odds | 'Print Odds' >> beam.Map(print)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d25498",
   "metadata": {},
   "source": [
    "## Example of branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71837fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "class RegionSplit(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        #return [(int(regionid), regionname)] # ParDo's need to return a list\n",
    "        yield (int(regionid), regionname) # Can also use yield instead of returning a list\n",
    "\n",
    "regionsfilename = 'regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(regionsfilename)\n",
    "          | 'Split' >> beam.ParDo(RegionSplit())\n",
    "    )\n",
    "    # Branch 1\n",
    "    (regions \n",
    "         | 'Lowercase regions' >> beam.Map(lambda x : (x[0] * 100, x[1].lower()))\n",
    "         | 'Write' >> WriteToText('regions2.out')\n",
    "    )\n",
    "    \n",
    "    (regions \n",
    "         | 'Uppercase regions' >> beam.Map(lambda x : (x[0] * 10, x[1].upper()))\n",
    "         | 'Print' >> beam.Map(print)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14abc43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat regions2*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1cdf42",
   "metadata": {},
   "source": [
    "## WithKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class TerritorySplit(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield (territoryid, territoryname, regionid)\n",
    "\n",
    "territoriesfilename = 'territories.csv'\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText(territoriesfilename)\n",
    "                    | 'Split Territories' >> beam.ParDo(TerritorySplit())\n",
    "                    | 'Territories With Keys' >> beam.util.WithKeys(lambda x : x[2])\n",
    "#                    | 'With Keys Manually' >> beam.Map(lambda x : (x[2], x))\n",
    "                  )\n",
    "    territories | 'Print KV' >> beam.Map(print)\n",
    "    territories | beam.util.Keys() | 'Print Keys' >> beam.Map(print)\n",
    "    territories | beam.util.Values() | 'Print Values' >> beam.Map(print)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db37a71",
   "metadata": {},
   "source": [
    "## GroupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ca963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class TerritorySplit(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield (territoryid, territoryname, regionid)\n",
    "\n",
    "territoriesfilename = 'territories.csv'\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Split Territories' >> beam.ParDo(TerritorySplit())\n",
    "                    | 'Territories With Keys' >> beam.util.WithKeys(lambda x : x[2])\n",
    "                    | 'Group Territories' >> beam.GroupByKey() \n",
    "                    | 'Print Territories' >> beam.Map(print)\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d565fe19",
   "metadata": {},
   "source": [
    "## Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc1b79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    lines1 = p | 'Create 1' >> beam.Create(['one', 'two', 'three', 'four'])\n",
    "    lines2 = p | 'Create 2' >> beam.Create(['alpha', 'beta', 'gamma', 'delta'])\n",
    "\n",
    "    merged = ((lines1, lines2) | 'Merge PCollections' >> beam.Flatten())\n",
    "    merged | beam.Map(print)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb08f7f",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cc847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    data = (\n",
    "        p | 'Create' >> beam.Create([('a', 10), ('a', 20), ('b', 30), ('b', 40), ('c', 50), ('a', 60)])\n",
    "          | 'Combine' >> beam.CombinePerKey(sum)\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3816f14",
   "metadata": {},
   "source": [
    "## Custom Combine Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e21786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mport apache_beam as beam\n",
    "\n",
    "class CustomCombine(beam.CombineFn):\n",
    "\n",
    "  def create_accumulator(self):\n",
    "    return {}\n",
    "\n",
    "  def add_input(self, accumulator, input):\n",
    "    k, v = input\n",
    "    x, y, z = accumulator.get(k, (0, 0, 0))\n",
    "\n",
    "    # take the max for the first element of the tuple and sum the second element and count for the third\n",
    "    accumulator[k] = (v[0] if v[0] > x else x, y + v[1], z + 1)\n",
    "    return accumulator\n",
    "\n",
    "  def merge_accumulators(self, accumulators):\n",
    "    merged = {}\n",
    "    for accum in accumulators:\n",
    "      for k, v in accum.items():\n",
    "        x, y, z = merged.get(k, (0, 0, 0))\n",
    "        merged[k] = (v[0] if v[0] > x else x, y + v[1], z + v[2])\n",
    "    return merged\n",
    "\n",
    "  def extract_output(self, accumulator):\n",
    "    # return the max, the sum, the count and the average for the key\n",
    "    return {k : (v[0], v[1], v[2], v[1]/v[2]) for k, v in accumulator.items()}\n",
    "    return accumulator\n",
    "    \n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    data = (\n",
    "        p | 'Create' >> beam.Create([('a', (1, 10)), ('a', (2, 20)), ('b', (3, 30)), ('b', (4, 40)), ('c', (5, 50)), ('a', (6, 60))])\n",
    "          | 'Combine' >> beam.CombineGlobally(CustomCombine())\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3541fd",
   "metadata": {},
   "source": [
    "## Map vs FlatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962a004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "  plants = (\n",
    "      pipeline\n",
    "      | 'Gardening plants' >> beam.Create(['Strawberry,Carrot,Eggplant','Tomato,Potato'])\n",
    "      | 'Split words' >> beam.Map(lambda x : x.split(','))\n",
    "#      | 'Split words' >> beam.FlatMap(lambda x : x.split(','))\n",
    "      | beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fcc572",
   "metadata": {},
   "source": [
    "## Side Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9467de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import AsSingleton, AsDict\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class TerritorySplitTuple(beam.DoFn):\n",
    "    # split territory into KV pair of (regionid, (territoryid, territoryname))\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "        \n",
    "                \n",
    "class LookupRegion(beam.DoFn):\n",
    "    def process(self, element, uppercase = 0):\n",
    "        lookuptable = {1:'North', 2:'South', 3:'East', 4:'West'}\n",
    "        territoryid, territoryname, regionid = element\n",
    "        region = lookuptable.get(regionid, 'No Region')\n",
    "        if uppercase == 1:\n",
    "            region = region.upper()\n",
    "        yield(territoryid, territoryname, regionid, region)\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "          | 'Split Territories' >> beam.ParDo(TerritorySplitTuple())\n",
    "    )\n",
    "    \n",
    "    lookup = (\n",
    "        territories\n",
    "#        | beam.ParDo(LookupRegion())\n",
    "        | beam.ParDo(LookupRegion(), uppercase = 1 ) \n",
    "        #beam.pvalue.AsSingleton(int(1)))\n",
    "        | 'Print Loopup' >> beam.Map(print)\n",
    "    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab06932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import AsList\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "\n",
    "class RegionSplitDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid': int(regionid), 'regionname': regionname.title()}\n",
    "\n",
    "class TerritorySplitTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "        \n",
    "                \n",
    "class LookupRegion(beam.DoFn):\n",
    "    def process(self, element, lookuptable = [{'regionid':1, 'regionname':'North'}, {'regionid':2, 'regionname':'South'}]):\n",
    "        territoryid, territoryname, regionid = element\n",
    "        lookup = {e['regionid'] : e['regionname'] for e in lookuptable }\n",
    "        yield(territoryid, territoryname, regionid, lookup.get(regionid, 'No Region'))\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read Regions' >> ReadFromText('regions.csv')\n",
    "          | 'Split Regions' >> beam.ParDo(RegionSplitDict())\n",
    "#          | 'Print Regions' >> beam.Map(print)\n",
    "    )\n",
    "\n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "          | 'Split Territories' >> beam.ParDo(TerritorySplitTuple())\n",
    "#          | 'Print Territories' >> beam.Map(print)\n",
    "    )\n",
    "    \n",
    "    lookup = (\n",
    "        territories\n",
    "        | beam.ParDo(LookupRegion(), lookuptable = beam.pvalue.AsList(regions))\n",
    "        | 'Print Loopup' >> beam.Map(print)\n",
    "    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d73d66d",
   "metadata": {},
   "source": [
    "## Simulate an Outer Join with CoGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2f0fbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'territoryid': 1581, 'territoryname': 'Westboro', 'regionid': 1, 'regionname': 'Eastern'}\n",
      "{'territoryid': 1730, 'territoryname': 'Bedford', 'regionid': 1, 'regionname': 'Eastern'}\n",
      "{'territoryid': 1833, 'territoryname': 'Georgetow', 'regionid': 1, 'regionname': 'Eastern'}\n",
      "{'territoryid': 2116, 'territoryname': 'Boston', 'regionid': 1, 'regionname': 'Eastern'}\n",
      "{'territoryid': 2139, 'territoryname': 'Cambridge', 'regionid': 1, 'regionname': 'Eastern'}\n",
      "{'territoryid': 2184, 'territoryname': 'Braintree', 'regionid': 1, 'regionname': 'Eastern'}\n",
      "{'territoryid': 2903, 'territoryname': 'Providence', 'regionid': 1, 'regionname': 'Eastern'}\n",
      "{'territoryid': 6897, 'territoryname': 'Wilton', 'regionid': 1, 'regionname': 'Eastern'}\n",
      "{'territoryid': 7960, 'territoryname': 'Morristown', 'regionid': 1, 'regionname': 'Eastern'}\n",
      "{'territoryid': 8837, 'territoryname': 'Edison', 'regionid': 1, 'regionname': 'Eastern'}\n",
      "{'territoryid': 10019, 'territoryname': 'New York', 'regionid': 1, 'regionname': 'Eastern'}\n",
      "{'territoryid': 10038, 'territoryname': 'New York', 'regionid': 1, 'regionname': 'Eastern'}\n",
      "{'territoryid': 11747, 'territoryname': 'Mellvile', 'regionid': 1, 'regionname': 'Eastern'}\n",
      "{'territoryid': 14450, 'territoryname': 'Fairport', 'regionid': 1, 'regionname': 'Eastern'}\n",
      "{'territoryid': 19713, 'territoryname': 'Neward', 'regionid': 1, 'regionname': 'Eastern'}\n",
      "{'territoryid': 20852, 'territoryname': 'Rockville', 'regionid': 1, 'regionname': 'Eastern'}\n",
      "{'territoryid': 27403, 'territoryname': 'Greensboro', 'regionid': 1, 'regionname': 'Eastern'}\n",
      "{'territoryid': 27511, 'territoryname': 'Cary', 'regionid': 1, 'regionname': 'Eastern'}\n",
      "{'territoryid': 40222, 'territoryname': 'Louisville', 'regionid': 1, 'regionname': 'Eastern'}\n",
      "{'territoryid': 60179, 'territoryname': 'Hoffman Estates', 'regionid': 2, 'regionname': 'Western'}\n",
      "{'territoryid': 60601, 'territoryname': 'Chicago', 'regionid': 2, 'regionname': 'Western'}\n",
      "{'territoryid': 80202, 'territoryname': 'Denver', 'regionid': 2, 'regionname': 'Western'}\n",
      "{'territoryid': 80909, 'territoryname': 'Colorado Springs', 'regionid': 2, 'regionname': 'Western'}\n",
      "{'territoryid': 85014, 'territoryname': 'Phoenix', 'regionid': 2, 'regionname': 'Western'}\n",
      "{'territoryid': 85251, 'territoryname': 'Scottsdale', 'regionid': 2, 'regionname': 'Western'}\n",
      "{'territoryid': 90405, 'territoryname': 'Santa Monica', 'regionid': 2, 'regionname': 'Western'}\n",
      "{'territoryid': 94025, 'territoryname': 'Menlo Park', 'regionid': 2, 'regionname': 'Western'}\n",
      "{'territoryid': 94105, 'territoryname': 'San Francisco', 'regionid': 2, 'regionname': 'Western'}\n",
      "{'territoryid': 95008, 'territoryname': 'Campbell', 'regionid': 2, 'regionname': 'Western'}\n",
      "{'territoryid': 95054, 'territoryname': 'Santa Clara', 'regionid': 2, 'regionname': 'Western'}\n",
      "{'territoryid': 95060, 'territoryname': 'Santa Cruz', 'regionid': 2, 'regionname': 'Western'}\n",
      "{'territoryid': 98004, 'territoryname': 'Bellevue', 'regionid': 2, 'regionname': 'Western'}\n",
      "{'territoryid': 98052, 'territoryname': 'Redmond', 'regionid': 2, 'regionname': 'Western'}\n",
      "{'territoryid': 98104, 'territoryname': 'Seattle', 'regionid': 2, 'regionname': 'Western'}\n",
      "{'territoryid': 3049, 'territoryname': 'Hollis', 'regionid': 3, 'regionname': 'Northern'}\n",
      "{'territoryid': 3801, 'territoryname': 'Portsmouth', 'regionid': 3, 'regionname': 'Northern'}\n",
      "{'territoryid': 19428, 'territoryname': 'Philadelphia', 'regionid': 3, 'regionname': 'Northern'}\n",
      "{'territoryid': 44122, 'territoryname': 'Beachwood', 'regionid': 3, 'regionname': 'Northern'}\n",
      "{'territoryid': 45839, 'territoryname': 'Findlay', 'regionid': 3, 'regionname': 'Northern'}\n",
      "{'territoryid': 48075, 'territoryname': 'Southfield', 'regionid': 3, 'regionname': 'Northern'}\n",
      "{'territoryid': 48084, 'territoryname': 'Troy', 'regionid': 3, 'regionname': 'Northern'}\n",
      "{'territoryid': 48304, 'territoryname': 'Bloomfield Hills', 'regionid': 3, 'regionname': 'Northern'}\n",
      "{'territoryid': 53404, 'territoryname': 'Racine', 'regionid': 3, 'regionname': 'Northern'}\n",
      "{'territoryid': 55113, 'territoryname': 'Roseville', 'regionid': 3, 'regionname': 'Northern'}\n",
      "{'territoryid': 55439, 'territoryname': 'Minneapolis', 'regionid': 3, 'regionname': 'Northern'}\n",
      "{'territoryid': 29202, 'territoryname': 'Columbia', 'regionid': 4, 'regionname': 'Southern'}\n",
      "{'territoryid': 30346, 'territoryname': 'Atlanta', 'regionid': 4, 'regionname': 'Southern'}\n",
      "{'territoryid': 31406, 'territoryname': 'Savannah', 'regionid': 4, 'regionname': 'Southern'}\n",
      "{'territoryid': 32859, 'territoryname': 'Orlando', 'regionid': 4, 'regionname': 'Southern'}\n",
      "{'territoryid': 33607, 'territoryname': 'Tampa', 'regionid': 4, 'regionname': 'Southern'}\n",
      "{'territoryid': 72716, 'territoryname': 'Bentonville', 'regionid': 4, 'regionname': 'Southern'}\n",
      "{'territoryid': 75234, 'territoryname': 'Dallas', 'regionid': 4, 'regionname': 'Southern'}\n",
      "{'territoryid': 78759, 'territoryname': 'Austin', 'regionid': 4, 'regionname': 'Southern'}\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "class RegionSplitDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid':int(regionid), 'regionname':regionname.title()}\n",
    "\n",
    "class TerritorySplitDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield {'territoryid':int(territoryid), 'territoryname' : territoryname, 'regionid':int(regionid)}\n",
    "\n",
    "class UnnestCoGrouped(beam.DoFn):\n",
    "    def process(self, item, child_pipeline, parent_pipeline):\n",
    "        k, v = item\n",
    "        child_dict = v[child_pipeline]\n",
    "        parent_dict = v[parent_pipeline]\n",
    "        for child in child_dict:\n",
    "            try:\n",
    "                child.update(parent_dict[0])\n",
    "                yield child\n",
    "            except IndexError:\n",
    "                yield child\n",
    "\n",
    "class LeftJoin(beam.PTransform):\n",
    "    def __init__(self, parent_pipeline_name, parent_data, parent_key, child_pipeline_name, child_data,  child_key):\n",
    "        self.parent_pipeline_name = parent_pipeline_name\n",
    "        self.parent_data = parent_data\n",
    "        self.parent_key = parent_key\n",
    "        self.child_pipeline_name = child_pipeline_name\n",
    "        self.child_data = child_data\n",
    "        self.child_key = child_key\n",
    "\n",
    "    def expand(self, pcols):\n",
    "        def _format_as_common_key_tuple(child_dict, child_key):\n",
    "            return (child_dict[child_key], child_dict)\n",
    "\n",
    "        return ({\n",
    "                pipeline_name: pcol1 | f'Convert to ({self.parent_key} = {self.child_key}, object) for {pipeline_name}' \n",
    "                >> beam.Map(_format_as_common_key_tuple, self.child_key)\n",
    "                for (pipeline_name, pcol1) in pcols.items()}\n",
    "                | f'CoGroupByKeey {pcols.keys()}' >> beam.CoGroupByKey()\n",
    "                | 'Unnest Cogrouped' >> beam.ParDo(UnnestCoGrouped(), self.child_pipeline_name, self.parent_pipeline_name)\n",
    "        )\n",
    "        \n",
    "regionsfilename = 'regions.csv'\n",
    "territoriesfilename = 'territories.csv'\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "              p | 'Read Regions' >> ReadFromText(regionsfilename)\n",
    "                | 'Split Regions' >> beam.ParDo(RegionSplitDict())\n",
    "                #| 'Print Regions' >> beam.Map(print)\n",
    "              )\n",
    "        \n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Split Territories' >> beam.ParDo(TerritorySplitDict())\n",
    "                    #| 'Print Territories' >> beam.Map(print)\n",
    "                  )\n",
    "\n",
    "    leftjoin = {'regions':regions, 'territories':territories} | LeftJoin('regions', regions, 'regionid', 'territories', territories, 'regionid')\n",
    "    leftjoin | 'print left join' >> beam.Map(print)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c038ed",
   "metadata": {},
   "source": [
    "## BeamSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e12aeb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                                            TAG       IMAGE ID       CREATED        SIZE\n",
      "gcr.io/deeplearning-platform-release/beam-notebooks   latest    86ec551d0d4d   3 days ago     12.4GB\n",
      "gcr.io/deeplearning-platform-release/base-cu101       latest    99cf5f3eee01   10 days ago    8.38GB\n",
      "gcr.io/deeplearning-platform-release/base-cu100       latest    afa32b51d431   10 days ago    7.77GB\n",
      "gcr.io/deeplearning-platform-release/base-cpu         latest    257fdd9e048b   10 days ago    4.98GB\n",
      "gcr.io/inverting-proxy/agent                          <none>    fe507176d0e6   2 months ago   1.73GB\n",
      "apache/beam_java11_sdk                                latest    0afcbaf4496f   2 months ago   736MB\n",
      "apache/beam_java8_sdk                                 2.28.0    14eadea78b3b   2 months ago   732MB\n",
      "apache/beam_java8_sdk                                 latest    14eadea78b3b   2 months ago   732MB\n",
      "apache/beam_java_sdk                                  2.25.0    7a355303c58f   6 months ago   723MB\n"
     ]
    }
   ],
   "source": [
    "! docker images\n",
    "#! docker pull apache/beam_java11_sdk \n",
    "#! docker pull apache/beam_java8_sdk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79533e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent(parent_id=1, parent_name='One')\n",
      "Parent(parent_id=2, parent_name='Two')\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam import coders\n",
    "from apache_beam.transforms.sql import SqlTransform\n",
    "\n",
    "import typing\n",
    "\n",
    "\n",
    "class Parent(typing.NamedTuple):\n",
    "    parent_id: int\n",
    "    parent_name: str\n",
    "beam.coders.registry.register_coder(Parent, beam.coders.RowCoder)\n",
    "\n",
    "class Child(typing.NamedTuple):\n",
    "    child_name: str\n",
    "    parent_id: int\n",
    "beam.coders.registry.register_coder(Child, beam.coders.RowCoder)\n",
    "        \n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    parent = (\n",
    "            p | 'create parent' >> beam.Create([(1, 'One'), (2, 'Two')])\n",
    "              | 'map parent' >> beam.Map(lambda x : Parent(parent_id = x[0], parent_name = x[1])).with_output_types(Parent)\n",
    "              | 'print parent' >> beam.Map(print)\n",
    "    )\n",
    "\n",
    "    child = (\n",
    "            p | 'create child' >> beam.Create([('Uno', 1), ('Due', 2), ('Eins', 1), ('Una', 1), ('Dos', 2)])\n",
    "              | 'map child' >> beam.Map(lambda x : Child(child_name = x[0], parent_id = x[1])).with_output_types(Child)\n",
    "#              | 'print child' >> beam.Map(print)\n",
    "              | 'sql child' >> SqlTransform(\"\"\"SELECT 10 * parent_id as parent_id, upper(child_name) as child_name from PCOLLECTION\"\"\")\n",
    "              | 'print map' >> beam.Map(lambda x : f'{x.parent_id} = {x.child_name}')\n",
    "#               | 'sql child' >> SqlTransform(\"\"\"SELECT parent_id, count(*) as cnt from PCOLLECTION GROUP BY parent_id\"\"\")\n",
    "#               | 'print map' >> beam.Map(lambda x : f'{x.parent_id} = {x.cnt}')\n",
    "              | 'print sql' >> beam.Map(print)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f053da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile beamsql1.py\n",
    "# This code is not running in the notebook\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam import coders\n",
    "from apache_beam.transforms.sql import SqlTransform\n",
    "\n",
    "import typing\n",
    "import json\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    parent = (\n",
    "            p | 'create parent' >> beam.Create([(1, 'One'), (2, 'Two')])\n",
    "              | 'map parent' >> beam.Map(lambda x : beam.Row(parent_id = x[0], parent_name = x[1]))\n",
    "#              | 'print parent' >> beam.Map(print)\n",
    "    )\n",
    "\n",
    "    child = (\n",
    "            p | 'create child' >> beam.Create([('Uno', 1), ('Due', 2), ('Eins', 1)])\n",
    "              | 'map child' >> beam.Map(lambda x : beam.Row(child_name = x[0], parent_id = x[1]))\n",
    "#              | 'print child' >> beam.Map(print)\n",
    "\n",
    "    )\n",
    "    \n",
    "    ( {'parent': parent, 'child' : child} | SqlTransform(\"\"\"SELECT p.parent_id, p.parent_name, c.child_name \n",
    "    FROM parent as p \n",
    "    INNER JOIN child as c ON p.parent_id = c.parent_id\"\"\")\n",
    "    | 'map join' >> beam.Map(lambda x : f'{x.parent_id} {x.parent_name} {x.child_name}')\n",
    "    | 'print join' >> beam.Map(print)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ed2f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile beamsql2.py\n",
    "# This code is not running in the notebook\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam import coders\n",
    "from apache_beam.transforms.sql import SqlTransform\n",
    "\n",
    "import typing\n",
    "import json\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "\n",
    "coders.registry.register_coder(Territory, coders.RowCoder)\n",
    "        \n",
    "class TerritorySplitClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield Territory(int(territoryid), territoryname.title(), int(regionid))\n",
    "\n",
    "        \n",
    "territoriesfilename = 'territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Split Territories' >> beam.ParDo(TerritorySplitClass()).with_output_types(Territory)\n",
    "                    | 'SQL Territories' >> SqlTransform(\"\"\"SELECT regionid, count(*) as `cnt` FROM PCOLLECTION GROUP BY regionid\"\"\")\n",
    "                    | 'Map Territories for Print' >> beam.Map(lambda x : f'{x.regionid} - {x.cnt}')\n",
    "                    | beam.Map(print)\n",
    "                    )\n",
    "    \n",
    "#https://www.youtube.com/watch?v=zx4p-UNSmrA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32763e50",
   "metadata": {},
   "source": [
    "## DoFn Lifecycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1782ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import AsSingleton, AsDict\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class TerritorySplitTuple(beam.DoFn):\n",
    "    # split territory into KV pair of (regionid, (territoryid, territoryname))\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "        \n",
    "                \n",
    "class LookupRegion(beam.DoFn):\n",
    "    def setup(self):\n",
    "        self.lookup = {1:'North', 2:'South', 3:'East', 4:'West'}\n",
    "        print('setup')\n",
    "        \n",
    "    def start_bundle(self):\n",
    "        print('start bundle')\n",
    "        \n",
    "    def process(self, element, uppercase = 0):\n",
    "        #lookuptable = {1:'North', 2:'South', 3:'East', 4:'West'}\n",
    "        territoryid, territoryname, regionid = element\n",
    "        region = self.lookup.get(regionid, 'No Region')\n",
    "        if uppercase == 1:\n",
    "            region = region.upper()\n",
    "        yield(territoryid, territoryname, regionid, region)\n",
    "        \n",
    "    def finish_bundle(self):\n",
    "        print('finish bundle')\n",
    "\n",
    "    def teardown(self):\n",
    "        print('teardown')\n",
    "        del self.lookup\n",
    "    \n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "          | 'Split Territories' >> beam.ParDo(TerritorySplitTuple())\n",
    "    )\n",
    "    \n",
    "    lookup = (\n",
    "        territories\n",
    "        | beam.ParDo(LookupRegion(), uppercase = 1 ) \n",
    "        | 'Print Loopup' >> beam.Map(print)\n",
    "    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad515c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import AsList\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "\n",
    "class RegionSplitDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid': int(regionid), 'regionname': regionname.title()}\n",
    "\n",
    "class TerritorySplitTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "        \n",
    "class LookupRegion(beam.DoFn):\n",
    "    def __init__(self):\n",
    "        print('init')\n",
    "        #self.lookup = {1:'north', 2:'south', 3:'east', 4:'west'}\n",
    "        #self.init_semaphore = False\n",
    "\n",
    "    def called_once(self, lookuptable):\n",
    "        print('called_once')\n",
    "        self.lookup = { e['regionid'] : e['regionname'] for e in lookuptable }\n",
    "        self.init_semaphore = False\n",
    "\n",
    "    def setup(self):\n",
    "        print('setup')\n",
    "        self.init_semaphore = True\n",
    "        #self.lookup = {1:'NORTH', 2:'South', 3:'East', 4:'West'}\n",
    "\n",
    "    def start_bundle(self):\n",
    "        print('start bundle')\n",
    "\n",
    "    def process(self, element, lookuptable = [{'regionid':1, 'regionname':'north'}, {'regionid':2, 'regionname':'south'}]):\n",
    "        if self.init_semaphore:\n",
    "            self.called_once(lookuptable)\n",
    "        territoryid, territoryname, regionid = element\n",
    "        yield(territoryid, territoryname, regionid, self.lookup.get(regionid, 'No Region'))\n",
    "\n",
    "    def finish_bundle(self):\n",
    "        print('finish bundle')\n",
    "\n",
    "    def teardown(self):\n",
    "        print('teardown')\n",
    "        del self.lookup\n",
    "        del self.init_semaphore\n",
    "                \n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "    regions = (\n",
    "        p | 'Read Regions' >> ReadFromText('regions.csv')\n",
    "          | 'Split Regions' >> beam.ParDo(RegionSplitDict())\n",
    "        )\n",
    "\n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "          | 'Split Territories' >> beam.ParDo(TerritorySplitTuple())\n",
    "#          | 'Print Territories' >> beam.Map(print)\n",
    "    )\n",
    "#     lookup = (\n",
    "#             territories\n",
    "#             | beam.ParDo(LookupRegion())\n",
    "#             | 'Print lookup' >> beam.Map(print)\n",
    "#     )\n",
    "    \n",
    "    lookup = (\n",
    "        territories\n",
    "        | beam.ParDo(LookupRegion(), lookuptable = beam.pvalue.AsList(regions))\n",
    "        | 'Print Loopup' >> beam.Map(print)\n",
    "    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492d33b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ae3438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15649fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0f33aa4",
   "metadata": {},
   "source": [
    "## Code below is not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4294bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "from apache_beam.transforms.sql import SqlTransform\n",
    "#from collections import namedtuple\n",
    "from typing import NamedTuple\n",
    "from apache_beam import coders\n",
    "\n",
    "\n",
    "RegionSchema = namedtuple(\"RegionSchema\", (\"regionid\", \"regionname\"))\n",
    "#coders.registry.register_coder(RegionSchema, coders.RowCoder)\n",
    "class RegionSplitSchema(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield RegionSchema(int(regionid), regionname.title())\n",
    "\n",
    "#TerritorySchema = namedtuple(\"TerritorySchema\", (\"territoryid\", \"territoryname\", \"regionid\"))\n",
    "TerritorySchema = NamedTuple(\"TerritorySchema\", [(\"territoryid\", int), (\"territoryname\", str), (\"regionid\", int)])\n",
    "coders.registry.register_coder(TerritorySchema, coders.RowCoder)\n",
    "class TerritorySplitSchema(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield TerritorySchema(int(territoryid), territoryname.title(), int(regionid))\n",
    "\n",
    "        \n",
    "# class TerritorySplitNamedTuple(beam.DoFn):\n",
    "#     def process(self, element):\n",
    "#         territoryid, territoryname, regionid = element.split(',')\n",
    "#         yield {'territoryid':int(territoryid), 'territoryname' : territoryname, 'regionid':int(regionid)}\n",
    "\n",
    "regionsfilename = 'regions.csv'\n",
    "territoriesfilename = 'territories.csv'\n",
    "print('Start')\n",
    "with beam.Pipeline() as p:\n",
    "#     regions = (\n",
    "#               p | 'Read Regions' >> ReadFromText(regionsfilename)\n",
    "#                 | 'Split Regions' >> beam.ParDo(RegionSplitSchema()).with_output_types(RegionSchema)\n",
    "#               )\n",
    "        \n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Split Territories' >> beam.ParDo(TerritorySplitSchema()) #.with_output_types(TerritorySchema)\n",
    "#                    | beam.Map(lambda x: TerritorySchema(x.territoryid, x.territoryname, x.regionid)).with_output_types(TerritorySchema)\n",
    "#         Map(lambda x: PythonSchema(f_int32=x)).with_output_types(PythonSchema)\n",
    "#                     | 'Apply Territories Schema' >> beam.Map(lambda x : beam.Row(territoryid = int(x.territoryid)\n",
    "#                                                                                  , territoryname = str(x.territoryname)\n",
    "#                                                                                  , regionid = int(x.regionid)))\n",
    "#                    | 'Convert to Dictionary' >> beam.Map(lambda row : {\"regionid\" : row.regionid, \"territoryid\" : row.territoryid, \"territoryname\" : row.territoryname})\n",
    "#                     | SqlTransform(\"\"\"\n",
    "#                         SELECT regionid, territoryname as name, territoryid \n",
    "#                         FROM PCOLLECTION\n",
    "#                         \"\"\")\n",
    "#                    | 'Split Territories' >> beam.ParDo(TerritorySplitSchema()).with_output_types(TerritorySchema)\n",
    "#                     | SqlTransform(\"\"\"\n",
    "#                         SELECT regionid, count(*) as territories\n",
    "#                         FROM PCOLLECTION\n",
    "#                         GROUP BY regionID\n",
    "#                         ORDER BY territories DESC\n",
    "#                         \"\"\")\n",
    "#                    | 'Convert to dictionary' >> beam.Map(lambda row : {\"regionid\": row.regionid, \"territories\": row.territories})\n",
    "                    \n",
    "#             })\n",
    "                  )\n",
    "\n",
    "#     regions | 'Print regions' >> beam.Map(print)\n",
    "    territories | 'Print territories' >> beam.Map(print)\n",
    "\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1778cbf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72469087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with beam.Pipeline() as pipeline:\n",
    "#     _ = (\n",
    "#         pipeline\n",
    "#         | beam.io.ReadFromPubSub(\n",
    "#             topic='projects/pubsub-public-data/topics/taxirides-realtime',\n",
    "#             timestamp_attribute=\"ts\").with_output_types(bytes)\n",
    "#         | \"Parse JSON payload\" >> beam.Map(json.loads)\n",
    "#         # Use beam.Row to create a schema-aware PCollection\n",
    "#         | \"Create beam Row\" >> beam.Map(\n",
    "#             lambda x: beam.Row(\n",
    "#                 ride_status=str(x['ride_status']),\n",
    "#                 passenger_count=int(x['passenger_count'])))\n",
    "#         # SqlTransform will computes result within an existing window\n",
    "#         | \"15s fixed windows\" >> beam.WindowInto(beam.window.FixedWindows(15))\n",
    "#         # Aggregate drop offs and pick ups that occur within each 15s window\n",
    "#         | SqlTransform(\n",
    "#             \"\"\"\n",
    "#              SELECT\n",
    "#                ride_status,\n",
    "#                COUNT(*) AS num_rides,\n",
    "#                SUM(passenger_count) AS total_passengers\n",
    "#              FROM PCOLLECTION\n",
    "#              WHERE NOT ride_status = 'enroute'\n",
    "#              GROUP BY ride_status\"\"\")\n",
    "#         # SqlTransform yields python objects with attributes corresponding to\n",
    "#         # the outputs of the query.\n",
    "#         # Collect those attributes, as well as window information, into a dict\n",
    "#         | \"Assemble Dictionary\" >> beam.Map(\n",
    "#             lambda row,\n",
    "#             window=beam.DoFn.WindowParam: {\n",
    "#                 \"ride_status\": row.ride_status,\n",
    "#                 \"num_rides\": row.num_rides,\n",
    "#                 \"total_passengers\": row.total_passengers,\n",
    "#                 \"window_start\": window.start.to_rfc3339(),\n",
    "#                 \"window_end\": window.end.to_rfc3339()\n",
    "#             })\n",
    "#         | \"Convert to JSON\" >> beam.Map(json.dumps)\n",
    "#         | \"UTF-8 encode\" >> beam.Map(lambda s: s.encode(\"utf-8\"))\n",
    "#         | beam.Map(print)\n",
    "#         #| beam.io.WriteToPubSub(topic=output_topic))\n",
    "#     )\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#   logging.getLogger().setLevel(logging.INFO)\n",
    "#   import argparse\n",
    "\n",
    "#   parser = argparse.ArgumentParser()\n",
    "#   parser.add_argument(\n",
    "#       '--output_topic',\n",
    "#       dest='output_topic',\n",
    "#       required=True,\n",
    "#       help=(\n",
    "#           'Cloud PubSub topic to write to (e.g. '\n",
    "#           'projects/my-project/topics/my-topic), must be created prior to '\n",
    "#           'running the pipeline.'))\n",
    "#   known_args, pipeline_args = parser.parse_known_args()\n",
    "\n",
    "#   run(known_args.output_topic, pipeline_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1029092b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435a888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookuptable = [{'regionid':1, 'regionname':'North'}, {'regionid':2, 'regionname':'South'}]\n",
    "lookup = {e['regionid'] : e['regionname'] for e in lookuptable }\n",
    "print(lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c2f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c68833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import AsIter, AsSingleton, AsList, AsDict\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "from apache_beam.io import ReadFromAvro, WriteToAvro\n",
    "from collections import namedtuple\n",
    "from apache_beam import coders\n",
    "from apache_beam.typehints.decorators import with_output_types\n",
    "\n",
    "\n",
    "class Region:\n",
    "    def __init__(self, regionid, regionname):\n",
    "        self.regionid = regionid\n",
    "        self.regionname = regionname\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'{self.regionid}|{self.regionname}'\n",
    "\n",
    "#     def encode(self, o):\n",
    "#         \"\"\"Encode to bytes with a trace that coder was used.\"\"\"\n",
    "#         # Our encoding prepends an 'x:' prefix.\n",
    "#         return b'x:%s' % o.encode('utf-8')\n",
    "\n",
    "#     def decode(self, s):\n",
    "#         # To decode, we strip off the prepended 'x:' prefix.\n",
    "#         s = s.decode('utf-8')\n",
    "#         #assert s[0:2] == 'x:'\n",
    "#         params = s[0:2].split('|')\n",
    "#         return Region(*params)\n",
    "\n",
    "#     def is_deterministic(self):\n",
    "#         # Since coded Player objects are used as keys below with\n",
    "#         # beam.CombinePerKey(sum), we require that this coder is deterministic\n",
    "#         # (i.e., two equivalent instances of the classes are encoded into the same\n",
    "#         # byte string) in order to guarantee consistent results.\n",
    "#         return True\n",
    "    \n",
    "class RegionSplitClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield Region(int(regionid), regionname.title())\n",
    "\n",
    "# class RegionCoder(coders.Coder):\n",
    "#   \"\"\"A custom coder for the RegionSchema\"\"\"\n",
    "#   def encode(self, o):\n",
    "#     \"\"\"Encode to bytes with a trace that coder was used.\"\"\"\n",
    "#     # Our encoding prepends an 'x:' prefix.\n",
    "#     return b'x:%s' % o.encode('utf-8')\n",
    "\n",
    "#   def decode(self, s):\n",
    "#     # To decode, we strip off the prepended 'x:' prefix.\n",
    "#     s = s.decode('utf-8')\n",
    "#     #assert s[0:2] == 'x:'\n",
    "#     params = s[0:2].split('|')\n",
    "#     return Region(*params)\n",
    "\n",
    "#   def is_deterministic(self):\n",
    "#     # Since coded Player objects are used as keys below with\n",
    "#     # beam.CombinePerKey(sum), we require that this coder is deterministic\n",
    "#     # (i.e., two equivalent instances of the classes are encoded into the same\n",
    "#     # byte string) in order to guarantee consistent results.\n",
    "#     return True\n",
    "# coders.registry.register_coder(Region, RegionCoder)\n",
    "\n",
    "# @with_output_types(typing.Tuple[Region, int])\n",
    "# def get_regions(descriptor):\n",
    "#   name, points = descriptor.split(',')\n",
    "#   return Player(name), int(points)\n",
    "\n",
    "\n",
    "# RegionSchema = namedtuple(\"RegionSchema\", (\"regionid\", \"regionname\"))\n",
    "# class RegionSplitSchema(beam.DoFn):\n",
    "#     def process(self, element):\n",
    "#         regionid, regionname = element.split(',')\n",
    "#         yield RegionSchema(int(regionid), regionname.title())\n",
    "\n",
    "class RegionSplitDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid': int(regionid), 'regionname': regionname.title()}\n",
    "\n",
    "\n",
    "class TerritorySplit(beam.DoFn):\n",
    "    # split territory into KV pair of (regionid, (territoryid, territoryname))\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "#        yield (int(regionid), (territoryid, territoryname.title())) \n",
    "        \n",
    "                \n",
    "def lookup_region(left, right):\n",
    "    territoryid, territoryname, regionid = left\n",
    "    yield territoryid, territoryname, regionid\n",
    "#    yield (territoryid, territorynme, regionid, right.get(regionid, 'No Region'))\n",
    "\n",
    "\n",
    "class LookupRegion(beam.DoFn):\n",
    "    def process(self, element, lookuptable = [{'regionid':1, 'regionname':'North'}, {'regionid':2, 'regionname':'South'}]):\n",
    "#        yield element\n",
    "        territoryid, territoryname, regionid = element\n",
    "        lookup = {e['regionid'] : e['regionname'] for e in lookuptable }\n",
    "        yield(territoryid, territoryname, regionid, lookup.get(regionid, 'No Region'))\n",
    "# #        yield (int(regionid), (territoryid, territoryname.title())) \n",
    "\n",
    "\n",
    "# def dummy(element):\n",
    "#     return element\n",
    "# #     regionid = element[0]\n",
    "# #     territoryid, territoryname = element[1]\n",
    "# #     return (territoryid, territoryname, regionid)\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read Regions' >> ReadFromText('regions.csv')\n",
    "          | 'Split Regions' >> beam.ParDo(RegionSplitDict())\n",
    "          #| 'Split Regions' >> beam.ParDo(RegionSplitClass())\n",
    "#          | 'Print Regions' >> beam.Map(print)\n",
    "    )\n",
    "\n",
    "#     regions = {1:\"North\", 2:\"South\", 3:\"East\", 4:\"West\"}\n",
    "#     regions = p | 'Create Regions' >> beam.Create([(1, 'North'), (2, 'South')])\n",
    "\n",
    "    \n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "          | 'Split Territories' >> beam.ParDo(TerritorySplit())\n",
    "#          | 'Print Territories' >> beam.Map(print)\n",
    "    )\n",
    "    \n",
    "    join = (\n",
    "        territories\n",
    "          #| 'Lookup Region' >> beam.Map(dummy)\n",
    "#          | 'Lookup Region' >> beam.Map(lookup_region, right = beam.pvalue.AsList(regions))\n",
    "#        | beam.ParDo(LookupRegion())\n",
    "        | beam.ParDo(LookupRegion(), lookuptable = beam.pvalue.AsList(regions))\n",
    "        | beam.Map(print)\n",
    "    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8209e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Beam 2.25.0 for Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
