{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f84cfa0",
   "metadata": {},
   "source": [
    "## Simple transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d72efc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One\n",
      "Two\n",
      "Three\n",
      "Four\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    lines = (\n",
    "        p | 'Create' >> beam.Create(['one', 'two', 'three', 'four'])\n",
    "          | 'Uppercase' >> beam.Map(str.title)\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d33982",
   "metadata": {},
   "source": [
    "## The pipe | is actually just an operator overload to call the apply method of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0dd1f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One\n",
      "Two\n",
      "Three\n",
      "Four\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "        lines = p | 'Create' >> beam.Create(['one', 'two', 'three', 'four'])\n",
    "        (lines \n",
    "            .apply(beam.Map(str.title), label = 'titlecase') \n",
    "            .apply(beam.Map(print), label='print')\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a75039",
   "metadata": {},
   "source": [
    "## Read from CSV and use Map.\n",
    "<br>\n",
    "<details><summary>Click for <b>Java</b></summary>\n",
    "<p> \n",
    "    \n",
    "```java\n",
    "\n",
    "package com.mypackage;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.PipelineResult;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.transforms.MapElements;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "\n",
    "\n",
    "public class Simple1 {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String regionsInputFileName = \"regions.csv\";\n",
    "        String regionsOutputFileName = \"output/regions\";\n",
    "\n",
    "        PCollection<String> regions = p\n",
    "            .apply(\"Read\", TextIO.read().from(regionsInputFileName))\n",
    "            .apply(\"Parse\", MapElements.into(TypeDescriptors.strings()).via((String element) -> element + \"*\"));\n",
    "        regions.apply(\"Write\", TextIO.write().to(regionsOutputFileName));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "}\n",
    "                       \n",
    "```\n",
    "</p>\n",
    "</details>\n",
    "<details><summary>Click for <b>Python</b></summary>\n",
    "<p>\n",
    "\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "\n",
    "def splitregion(line):\n",
    "    #1,North\n",
    "    return tuple(line.split(','))\n",
    "\n",
    "regionsfilename = 'regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(regionsfilename)\n",
    "#          | 'Parse' >> beam.Map(lambda x : tuple(x.split(',')))\n",
    "          | 'Parse' >> beam.Map(splitregion)\n",
    "          | 'Transform' >> beam.Map(lambda x : (int(x[0]) * 10, x[1].upper()))\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n",
    "    #p.run()\n",
    "```\n",
    "</p>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe33622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 'EASTERN')\n",
      "(20, 'WESTERN')\n",
      "(30, 'NORTHERN')\n",
      "(40, 'SOUTHERN')\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "\n",
    "def splitregion(line):\n",
    "    #1,North\n",
    "    return tuple(line.split(','))\n",
    "\n",
    "regionsfilename = 'regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(regionsfilename)\n",
    "#          | 'Parse' >> beam.Map(lambda x : tuple(x.split(',')))\n",
    "          | 'Parse' >> beam.Map(splitregion)\n",
    "          | 'Transform' >> beam.Map(lambda x : (int(x[0]) * 10, x[1].upper()))\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n",
    "    #p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae4355b",
   "metadata": {},
   "source": [
    "## Read from CSV and use ParDo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1360295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "\n",
    "class RegionParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        #return [(int(regionid), regionname)] # ParDo's need to return a list\n",
    "        yield (int(regionid), regionname) # Can also use yield instead of returning a list\n",
    "\n",
    "regionsfilename = 'regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(regionsfilename)\n",
    "          | 'Parse' >> beam.ParDo(RegionParseTuple())\n",
    "          | 'Write' >> WriteToText('regions.out')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7550bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat regions.out*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df71460f",
   "metadata": {},
   "source": [
    "## Template showing a full program that can read the command line args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e8cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A template to import the default package and parse the arguments\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import re, os\n",
    "\n",
    "#from past.builtins import unicode\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "class RegionParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield (int(regionid), regionname) # Can also use yield instead of returning a list\n",
    "\n",
    "def run(argv=None, save_main_session=True):\n",
    "  \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n",
    "  projectid = os.environ.get('PROJECT')\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--input',\n",
    "      dest='input',\n",
    "      default=f'gs://{projectid}/regions.csv',\n",
    "      help='Input file to process.')\n",
    "  parser.add_argument(\n",
    "      '--output',\n",
    "      dest='output',\n",
    "      default = f'gs://{projectid}/regions_output',      \n",
    "      help='Output file to write results to.')\n",
    "  known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "\n",
    "  # We use the save_main_session option because one or more DoFn's in this\n",
    "  # workflow rely on global context (e.g., a module imported at module level).\n",
    "  pipeline_options = PipelineOptions(pipeline_args)\n",
    "  pipeline_options.view_as(SetupOptions).save_main_session = save_main_session\n",
    "\n",
    "  # The pipeline will be run on exiting the with block.\n",
    "  with beam.Pipeline(options=pipeline_options) as p:\n",
    "    lines = p | 'Read' >> ReadFromText(known_args.input)\n",
    "    records = lines | 'Split' >> beam.ParDo(RegionParseTuple())\n",
    "    uppercase = records | 'Uppercase' >> beam.Map(lambda x : (int(x[0]), x[1].upper()))\n",
    "    uppercase | 'Write' >> WriteToText(known_args.output)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  logging.getLogger().setLevel(logging.INFO)\n",
    "  run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2022b65",
   "metadata": {},
   "source": [
    "## Example of how to create a split ParDo with multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3f29dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam import pvalue\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class OddEvenRegionParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        if int(regionid) % 2 == 0:\n",
    "            yield pvalue.TaggedOutput('Even', (int(regionid), regionname, 'Even'))\n",
    "        else:\n",
    "            yield pvalue.TaggedOutput('Odd', (int(regionid), regionname, 'Odd'))\n",
    "\n",
    "regionsfilename = 'regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    lines = p | 'Read' >> ReadFromText(regionsfilename)\n",
    "    evens, odds = lines | 'Parse' >> beam.ParDo(OddEvenRegionParseTuple()).with_outputs(\"Even\", \"Odd\")\n",
    "    \n",
    "    print('Evens')\n",
    "    evens | 'Print Evens' >> beam.Map(print)\n",
    "\n",
    "    print('Odds')\n",
    "    odds | 'Print Odds' >> beam.Map(print)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdc1c6a",
   "metadata": {},
   "source": [
    "## Example of branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e756d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "\n",
    "class RegionParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        #return [(int(regionid), regionname)] # ParDo's need to return a list\n",
    "        yield (int(regionid), regionname) # Can also use yield instead of returning a list\n",
    "\n",
    "regionsfilename = 'regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(regionsfilename)\n",
    "          | 'Parse' >> beam.ParDo(RegionParseTuple())\n",
    "    )\n",
    "    # Branch 1\n",
    "    (regions \n",
    "         | 'Lowercase regions' >> beam.Map(lambda x : (x[0] * 100, x[1].lower()))\n",
    "         | 'Write' >> WriteToText('regions2.out')\n",
    "    )\n",
    "    # Branch 2\n",
    "    (regions \n",
    "         | 'Uppercase regions' >> beam.Map(lambda x : (x[0] * 10, x[1].upper()))\n",
    "         | 'Print' >> beam.Map(print)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344b1de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat regions2*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d425ef0d",
   "metadata": {},
   "source": [
    "## WithKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4082e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield (int(territoryid), territoryname, int(regionid))\n",
    "\n",
    "territoriesfilename = 'territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText(territoriesfilename)\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "                    | 'Territories With Keys' >> beam.util.WithKeys(lambda x : x[2])\n",
    "#                    | 'With Keys Manually' >> beam.Map(lambda x : (x[2], x))\n",
    "                  )\n",
    "#    territories | 'Print KV' >> beam.Map(print)\n",
    "#    territories | beam.util.Keys() | 'Print Keys' >> beam.Map(print)\n",
    "#     territories | beam.util.Values() | 'Print Values' >> beam.Map(print)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb610f5",
   "metadata": {},
   "source": [
    "## GroupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0078a30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield (int(territoryid), territoryname, int(regionid))\n",
    "\n",
    "territoriesfilename = 'territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "                    | 'Territories With Keys' >> beam.util.WithKeys(lambda x : x[2])\n",
    "                    | 'Group Territories' >> beam.GroupByKey() \n",
    "#                    | 'Print Territories' >> beam.Map(print)\n",
    "                    | 'Write' >> WriteToText('territories_group.out')\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa18c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls territories_group.out*\n",
    "#! cat territories_group.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a7786",
   "metadata": {},
   "source": [
    "## Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    lines1 = p | 'Create 1' >> beam.Create(['one', 'two', 'three', 'four'])\n",
    "    lines2 = p | 'Create 2' >> beam.Create(['alpha', 'beta', 'gamma', 'delta'])\n",
    "\n",
    "    merged = ((lines1, lines2) | 'Merge PCollections' >> beam.Flatten())\n",
    "    merged | beam.Map(print)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766a332d",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826be531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    data = (\n",
    "        p | 'Create' >> beam.Create([('a', 10), ('a', 20), ('b', 30), ('b', 40), ('c', 50), ('a', 60)])\n",
    "          | 'Combine' >> beam.CombinePerKey(sum)\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d31e56",
   "metadata": {},
   "source": [
    "## Custom Combine Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "877760d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': (6, 90, 3, 30.0), 'b': (4, 70, 2, 35.0), 'c': (5, 50, 1, 50.0)}\n"
     ]
    }
   ],
   "source": [
    "#mport apache_beam as beam\n",
    "\n",
    "class CustomCombine(beam.CombineFn):\n",
    "\n",
    "  def create_accumulator(self):\n",
    "    return dict()\n",
    "\n",
    "  def add_input(self, accumulator, input):\n",
    "    k, v = input\n",
    "    x, y, z = accumulator.get(k, (0, 0, 0))\n",
    "\n",
    "    # take the max for the first element of the tuple and sum the second element and count for the third\n",
    "    accumulator[k] = (v[0] if v[0] > x else x, y + v[1], z + 1)\n",
    "    return accumulator\n",
    "\n",
    "  def merge_accumulators(self, accumulators):\n",
    "    merged = dict()\n",
    "    for accum in accumulators:\n",
    "      for k, v in accum.items():\n",
    "        x, y, z = merged.get(k, (0, 0, 0))\n",
    "        merged[k] = (v[0] if v[0] > x else x, y + v[1], z + v[2])\n",
    "    return merged\n",
    "\n",
    "  def extract_output(self, accumulator):\n",
    "    # return the max, the sum, the count and the average for the key\n",
    "    return {k : (v[0], v[1], v[2], v[1]/v[2]) for k, v in accumulator.items()}\n",
    "    return accumulator\n",
    "    \n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    data = (\n",
    "        p | 'Create' >> beam.Create([('a', (1, 10)), ('a', (2, 20)), \n",
    "                                     ('b', (3, 30)), ('c', (5, 50)), \n",
    "                                     ('b', (4, 40)), ('a', (6, 60))])\n",
    "          | 'Combine' >> beam.CombineGlobally(CustomCombine())\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb82ea5",
   "metadata": {},
   "source": [
    "## Map vs FlatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1049fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "  plants = (\n",
    "      pipeline\n",
    "      | 'Gardening plants' >> beam.Create(['Strawberry,Carrot,Eggplant','Tomato,Potato'])\n",
    "#      | 'Split words' >> beam.Map(lambda x : x.split(','))\n",
    "      | 'Split words' >> beam.FlatMap(lambda x : x.split(','))\n",
    "      | beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0fd79d",
   "metadata": {},
   "source": [
    "## Parsing the line as a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da16badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "\n",
    "class StartsWithSFilter(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        if element[1].startswith('S'):\n",
    "            yield element\n",
    "            \n",
    "territoriesfilename = 'territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(territoriesfilename)\n",
    "          | 'Parse' >> beam.ParDo(TerritoryParseTuple())\n",
    "          | 'Filter 1' >> beam.Filter(lambda x : x[2] % 2 == 0)\n",
    "#          | 'Filter 2' >> beam.Filter(lambda x : x[1].startswith('S'))\n",
    "          | 'Filter 2' >> beam.ParDo(StartsWithSFilter())\n",
    "          | 'Print' >> beam.Map(print)\n",
    "#          | 'Write' >> WriteToText('regions.out')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "\n",
    "class TerritoryParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield {'territoryid' : int(territoryid), 'territoryname' : territoryname, 'regionid'  : int(regionid)}\n",
    "\n",
    "class StartsWithSFilter(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        if element[1].startswith('S'):\n",
    "            yield element\n",
    "            \n",
    "territoriesfilename = 'territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(territoriesfilename)\n",
    "          | 'Parse' >> beam.ParDo(TerritoryParseDict())\n",
    "          | 'Filter 1' >> beam.Filter(lambda x : x['regionid'] % 2 == 0)\n",
    "          | 'Filter 2' >> beam.Filter(lambda x : x['territoryname'].startswith('S'))\n",
    "          | 'Print' >> beam.Map(print)\n",
    "#          | 'Write' >> WriteToText('regions.out')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f891dae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "import typing\n",
    "\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "beam.coders.registry.register_coder(Territory, beam.coders.RowCoder)\n",
    "        \n",
    "class TerritoryParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield Territory(int(territoryid), territoryname, int(regionid))\n",
    "\n",
    "class StartsWithSFilter(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        if element[1].startswith('S'):\n",
    "            yield element\n",
    "            \n",
    "territoriesfilename = 'territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(territoriesfilename)\n",
    "          | 'Parse' >> beam.ParDo(TerritoryParseClass())\n",
    "          | 'Filter 1' >> beam.Filter(lambda x : x.regionid % 2 == 0)\n",
    "          | 'Filter 2' >> beam.Filter(lambda x : x.territoryname.startswith('S'))\n",
    "          | 'Print' >> beam.Map(print)\n",
    "#          | 'Write' >> WriteToText('regions.out')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b2974e",
   "metadata": {},
   "source": [
    "## Side Inputs with a single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d472b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import AsSingleton, AsDict\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    # split territory into KV pair of (regionid, (territoryid, territoryname))\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "\n",
    "class LookupRegion(beam.DoFn):\n",
    "    def process(self, element, uppercase = 0):\n",
    "        lookuptable = {1:'North', 2:'South', 3:'East', 4:'West'}\n",
    "        territoryid, territoryname, regionid = element\n",
    "        region = lookuptable.get(regionid, 'No Region')\n",
    "        if uppercase == 1:\n",
    "            region = region.upper()\n",
    "        yield(territoryid, territoryname, regionid, region)\n",
    "        \n",
    "with beam.Pipeline() as p:\n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "          | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "    )\n",
    "    \n",
    "    # This is contrived, but let's just calculate a value that we can pass as a single side input to LookupRegion\n",
    "    minregion = (\n",
    "        territories\n",
    "          | 'Extract RegionID' >> beam.Map(lambda x : x[2])\n",
    "          | 'MaxTerritories' >> beam.CombineGlobally(lambda elements: min(elements or [None]))\n",
    "    )\n",
    "\n",
    "    lookup = (\n",
    "        territories\n",
    "#          | beam.ParDo(LookupRegion(), uppercase = 0 ) \n",
    "          | beam.ParDo(LookupRegion(), uppercase = beam.pvalue.AsSingleton(minregion) ) \n",
    "#          | beam.ParDo(LookupRegion(), uppercase = minregion ) \n",
    "          | 'Print Loopup' >> beam.Map(print)\n",
    "    )\n",
    "\n",
    "#    maxregion | 'Print Min' >> beam.Map(print)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e4d630",
   "metadata": {},
   "source": [
    "## Side input that is a lookup list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb52c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import AsList\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class RegionParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid': int(regionid), 'regionname': regionname.title()}\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "        \n",
    "                \n",
    "class LookupRegion(beam.DoFn):\n",
    "    def process(self, element, lookuptable = [{'regionid':1, 'regionname':'North'}, {'regionid':2, 'regionname':'South'}]):\n",
    "        territoryid, territoryname, regionid = element\n",
    "        # Becase the regions PCollection is a different shape, use the following comprehension to make it easier to do a lookup\n",
    "        lookup = {e['regionid'] : e['regionname'] for e in lookuptable } # {1:'North', 2:'South'}\n",
    "        yield(territoryid, territoryname, regionid, lookup.get(regionid, 'No Region'))\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read Regions' >> ReadFromText('regions.csv')\n",
    "          | 'Parse Regions' >> beam.ParDo(RegionParseDict())\n",
    "#          | 'Print Regions' >> beam.Map(print)\n",
    "    )\n",
    "\n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "          | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "#          | 'Print Territories' >> beam.Map(print)\n",
    "    )\n",
    "    \n",
    "    lookup = (\n",
    "        territories\n",
    "        | beam.ParDo(LookupRegion(), lookuptable = beam.pvalue.AsList(regions))\n",
    "        | 'Print Loopup' >> beam.Map(print)\n",
    "    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1310f0ab",
   "metadata": {},
   "source": [
    "## Create a nested repeating output\n",
    "### First create a dataset. Here is python code for the equivalent bq command of bq mk dataflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffae2de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset qwiklabs-gcp-04-4cf93802c378.dataflow1\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "# TODO(developer): Set dataset_id to the ID of the dataset to create.\n",
    "PROJECT_ID = 'qwiklabs-gcp-04-4cf93802c378'\n",
    "dataset_id = f\"{PROJECT_ID}.dataflow\" #.format(client.project)\n",
    "\n",
    "# Construct a full Dataset object to send to the API.\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "\n",
    "# TODO(developer): Specify the geographic location where the dataset should reside.\n",
    "dataset.location = \"US\"\n",
    "\n",
    "# Send the dataset to the API for creation, with an explicit timeout.\n",
    "# Raises google.api_core.exceptions.Conflict if the Dataset already\n",
    "# exists within the project.\n",
    "dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8385cc6",
   "metadata": {},
   "source": [
    "## Run the following in a bq query window to create the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc0eb4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery error in mk operation: Invalid schema entry:\n",
      "territories:ARRAY<STRUCT<territoryid:NUMERIC\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "create table dataflow.region_territory\n",
    "(regionid NUMERIC\n",
    ",regionname STRING\n",
    ",territories ARRAY<STRUCT<territoryid NUMERIC, territoryname STRING>>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaf71bc",
   "metadata": {},
   "source": [
    "## Need to manually create a schema for a nested repeating, cannot use a simple string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a3371ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.io.gcp.internal.clients import bigquery as bq\n",
    "region_territory_schema = bq.TableSchema()\n",
    "regionid = bq.TableFieldSchema()\n",
    "regionid.name='regionid'\n",
    "regionid.type='string'\n",
    "regionid.mode='required'\n",
    "region_territory_schema.fields.append(regionid)\n",
    "regionname = bq.TableFieldSchema()\n",
    "regionname.name='regionname'\n",
    "regionname.type='string'\n",
    "regionname.mode='required'\n",
    "region_territory_schema.fields.append(regionname)\n",
    "\n",
    "\n",
    "# A nested field\n",
    "territories = bq.TableFieldSchema()\n",
    "territories.name='territores'\n",
    "territories.type='record'\n",
    "territories.mode='nullable'\n",
    "\n",
    "\n",
    "territoryid = bq.TableFieldSchema()\n",
    "territoryid.name='territoryid'\n",
    "territoryid.type='string'\n",
    "territoryid.mode='required'\n",
    "territories.fields.append(territoryid)\n",
    "territoryname = bq.TableFieldSchema()\n",
    "territoryname.name='territoryname'\n",
    "territoryname.type='string'\n",
    "territoryname.mode='required'\n",
    "territories.fields.append(territoryname)\n",
    "\n",
    "region_territory_schema.fields.append(territories)\n",
    "\n",
    "#print(region_territory_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa5c07d",
   "metadata": {},
   "source": [
    "## The code here is tricky: \n",
    "### First parse the two tables into tuples, (regionid, regionname) & (regionid, {'territoryid':territoryid, 'territoryname':territoryname})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "95339773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class RegionParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield (int(regionid), regionname) # Can also use yield instead of returning a list\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    # split territory into KV pair of (regionid, (territoryid, territoryname))\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(regionid), {'territoryid': int(territoryid), 'territoryname':territoryname})\n",
    "\n",
    "regionsfilename = 'regions.csv'\n",
    "territoriesfilename = 'territories.csv'\n",
    "PROJECT_ID = 'qwiklabs-gcp-04-4cf93802c378'\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "              p | 'Read Regions' >> ReadFromText(regionsfilename)\n",
    "                | 'Parse Regions' >> beam.ParDo(RegionParseTuple())\n",
    "                #| 'Print Regions' >> beam.Map(print)\n",
    "              )\n",
    "        \n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "                    #| 'Print Territories' >> beam.Map(print)\n",
    "                  )\n",
    "    nested = ( \n",
    "        {'regions':regions, 'territories':territories} \n",
    "              | 'Nest territories into regions' >> beam.CoGroupByKey()\n",
    "              | 'Reshape to dict' >> beam.Map(lambda x : {'regionid': x[0], 'regionname': x[1]['regions'][0], \n",
    "                                                        'territories': x[1]['territories']})\n",
    "    )\n",
    "    nested | 'Write nested region_territory to BQ' >> beam.io.WriteToBigQuery('region_territory', dataset = 'dataflow'\n",
    "                                                                             , project = PROJECT_ID\n",
    "                                                                             , schema = region_territory_schema\n",
    "                                                                             , method = 'STREAMING_INSERTS'\n",
    "                                                                             )\n",
    "#    nested | 'Print' >> beam.Map(print)\n",
    "             \n",
    "#help(beam.io.WriteToBigQuery)    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bbdc03",
   "metadata": {},
   "source": [
    "## Simulate an Outer Join with CoGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424a249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class RegionParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid':int(regionid), 'regionname':regionname.title()}\n",
    "\n",
    "class TerritoryParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield {'territoryid':int(territoryid), 'territoryname' : territoryname, 'regionid':int(regionid)}\n",
    "\n",
    "class UnnestCoGrouped(beam.DoFn):\n",
    "    def process(self, item, child_pipeline, parent_pipeline):\n",
    "        k, v = item\n",
    "        child_dict = v[child_pipeline]\n",
    "        parent_dict = v[parent_pipeline]\n",
    "        for child in child_dict:\n",
    "            try:\n",
    "                child.update(parent_dict[0])\n",
    "                yield child\n",
    "            except IndexError:\n",
    "                yield child\n",
    "\n",
    "class LeftJoin(beam.PTransform):\n",
    "    def __init__(self, parent_pipeline_name, parent_data, parent_key, child_pipeline_name, child_data,  child_key):\n",
    "        self.parent_pipeline_name = parent_pipeline_name\n",
    "        self.parent_data = parent_data\n",
    "        self.parent_key = parent_key\n",
    "        self.child_pipeline_name = child_pipeline_name\n",
    "        self.child_data = child_data\n",
    "        self.child_key = child_key\n",
    "\n",
    "    def expand(self, pcols):\n",
    "        def _format_as_common_key_tuple(child_dict, child_key):\n",
    "            return (child_dict[child_key], child_dict)\n",
    "\n",
    "        return ({\n",
    "                pipeline_name: pcol1 | f'Convert to ({self.parent_key} = {self.child_key}, object) for {pipeline_name}' \n",
    "                >> beam.Map(_format_as_common_key_tuple, self.child_key)\n",
    "                for (pipeline_name, pcol1) in pcols.items()}\n",
    "                | f'CoGroupByKey {pcols.keys()}' >> beam.CoGroupByKey()\n",
    "                | 'Unnest Cogrouped' >> beam.ParDo(UnnestCoGrouped(), self.child_pipeline_name, self.parent_pipeline_name)\n",
    "        )\n",
    "        \n",
    "regionsfilename = 'regions.csv'\n",
    "territoriesfilename = 'territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "              p | 'Read Regions' >> ReadFromText(regionsfilename)\n",
    "                | 'Parse Regions' >> beam.ParDo(RegionParseDict())\n",
    "                #| 'Print Regions' >> beam.Map(print)\n",
    "              )\n",
    "        \n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseDict())\n",
    "                    #| 'Print Territories' >> beam.Map(print)\n",
    "                  )\n",
    "\n",
    "    leftjoin = {'regions':regions, 'territories':territories} | LeftJoin('regions', regions, 'regionid', 'territories', territories, 'regionid')\n",
    "    leftjoin | 'Print Left Join' >> beam.Map(print)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9873998a",
   "metadata": {},
   "source": [
    "(1, north) (2, south)\n",
    "\n",
    "10, 1    \n",
    "11, 1\n",
    "12, 2\n",
    "13, 2\n",
    "14, 1\n",
    "\n",
    "\n",
    "(1, [(10, 1), (11, 1), (14,1)], [(1, north)])\n",
    "(2, [(12, 2), (13,2)], [(2, South)])\n",
    "\n",
    "\n",
    "all region 1 end up on some worker from both the regions and territories\n",
    "\n",
    "\n",
    "[ pcol1, pcol2] | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1549d",
   "metadata": {},
   "source": [
    "## BeamSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker images\n",
    "#! docker pull apache/beam_java11_sdk \n",
    "#! docker pull apache/beam_java8_sdk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3220d73c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08050f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile beamsql1.py\n",
    "# This code is not running in the notebook\n",
    "# This example just uses a basic Row object\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam import coders\n",
    "from apache_beam.transforms.sql import SqlTransform\n",
    "\n",
    "import typing\n",
    "import json\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    parent = (\n",
    "            p | 'Create Parent' >> beam.Create([(1, 'One'), (2, 'Two')])\n",
    "              | 'Map Parent' >> beam.Map(lambda x : beam.Row(parent_id = x[0], parent_name = x[1]))\n",
    "    )\n",
    "\n",
    "    child = (\n",
    "            p | 'Create Child' >> beam.Create([('Uno', 1), ('Due', 2), ('Eins', 1), ('Una', 1), ('Dos', 2)])\n",
    "              | 'Map Child' >> beam.Map(lambda x : beam.Row(child_name = x[0], parent_id = x[1]))\n",
    "    )\n",
    "    \n",
    "    ( {'parent': parent, 'child' : child} \n",
    "         | SqlTransform(\"\"\"\n",
    "             SELECT p.parent_id, p.parent_name, c.child_name \n",
    "             FROM parent as p \n",
    "             INNER JOIN child as c ON p.parent_id = c.parent_id\n",
    "             \"\"\")\n",
    "        | 'Map Join' >> beam.Map(lambda x : f'{x.parent_id} {x.parent_name} {x.child_name}')\n",
    "        | 'Print Join' >> beam.Map(print)\n",
    "        )\n",
    "\n",
    "#     parent | 'print parent' >> beam.Map(print)\n",
    "#     child  | 'print child' >> beam.Map(print)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88e6c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile beamsql2.py\n",
    "# This code is not running in the notebook\n",
    "# This example uses a simple class to handle the schemas\n",
    "import apache_beam as beam\n",
    "from apache_beam import coders\n",
    "from apache_beam.transforms.sql import SqlTransform\n",
    "\n",
    "import typing\n",
    "\n",
    "\n",
    "class Parent(typing.NamedTuple):\n",
    "    parent_id: int\n",
    "    parent_name: str\n",
    "beam.coders.registry.register_coder(Parent, beam.coders.RowCoder)\n",
    "\n",
    "class Child(typing.NamedTuple):\n",
    "    child_name: str\n",
    "    parent_id: int\n",
    "beam.coders.registry.register_coder(Child, beam.coders.RowCoder)\n",
    "        \n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    parent = (\n",
    "            p | 'Create Parent' >> beam.Create([(1, 'One'), (2, 'Two')])\n",
    "              | 'Map Parent' >> beam.Map(lambda x : Parent(parent_id = x[0], parent_name = x[1])).with_output_types(Parent)\n",
    "              | 'Map for Print' >> beam.Map(print)\n",
    "    )\n",
    "\n",
    "    child = (\n",
    "            p | 'Create Child' >> beam.Create([('Uno', 1), ('Due', 2), ('Eins', 1), ('Una', 1), ('Dos', 2)])\n",
    "              | 'Map Child' >> beam.Map(lambda x : Child(child_name = x[0], parent_id = x[1])).with_output_types(Child)\n",
    "#               | 'SQL Child' >> SqlTransform(\"\"\"SELECT 10 * parent_id as parent_id, upper(child_name) as child_name from PCOLLECTION\"\"\")\n",
    "#               | 'Print Map' >> beam.Map(lambda x : f'{x.parent_id} = {x.child_name}')\n",
    "              | 'SQL Child' >> SqlTransform(\"\"\"SELECT parent_id, count(*) as cnt from PCOLLECTION GROUP BY parent_id\"\"\")\n",
    "              | 'Map for Print' >> beam.Map(lambda x : f'{x.parent_id} = {x.cnt}')\n",
    "              | 'Print SQL' >> beam.Map(print)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046adf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile beamsql3.py\n",
    "# This code is not running in the notebook\n",
    "# This example is like example 2 but for a real file \n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam import coders\n",
    "from apache_beam.transforms.sql import SqlTransform\n",
    "\n",
    "import typing\n",
    "import json\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "\n",
    "coders.registry.register_coder(Territory, coders.RowCoder)\n",
    "        \n",
    "@beam.typehints.with_output_types(Territory)\n",
    "class TerritoryParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield Territory(int(territoryid), territoryname.title(), int(regionid))\n",
    "\n",
    "        \n",
    "territoriesfilename = 'territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "#                    | 'Parse Territories' >> beam.ParDo(TerritoryParseClass()).with_output_types(Territory)\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseClass())\n",
    "                    | 'SQL Territories' >> SqlTransform(\"\"\"SELECT regionid, count(*) as `cnt` FROM PCOLLECTION GROUP BY regionid\"\"\")\n",
    "                    | 'Map Territories for Print' >> beam.Map(lambda x : f'{x.regionid} - {x.cnt}')\n",
    "                    | 'Print SQL' >> beam.Map(print)\n",
    "                    )\n",
    "    \n",
    "#https://www.youtube.com/watch?v=zx4p-UNSmrA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761f821",
   "metadata": {},
   "outputs": [],
   "source": [
    "territoriesfilename = 'territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseClass())\n",
    "                    | 'SQL Territories' >> SqlTransform(\"\"\"SELECT regionid, count(*) as `cnt` FROM PCOLLECTION GROUP BY regionid\"\"\")\n",
    "                    )\n",
    "\n",
    "                    territories | 'Write to BQ' >> beam.WriteToBQ()\n",
    "                    territories | 'Write TO File' >> beam.WriteToText()\n",
    "#    p.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b94756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "territoriesfilename = 'territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseClass())\n",
    "                  )\n",
    "                  \n",
    "                (territories | 'SQL Territories' >> SqlTransform(\"\"\"SELECT regionid, count(*) as `cnt` FROM PCOLLECTION GROUP BY regionid\"\"\")\n",
    "                            | 'Write to BQ' >> beam.WriteToBQ()\n",
    "                )\n",
    "                (territories | 'SQL Territories2' >> SqlTransform(\"\"\"SELECT territoryid, count(*) as `cnt` FROM PCOLLECTION GROUP BY regionid\"\"\")\n",
    "                            | 'Write to BQ' >> beam.WriteToBQ()\n",
    "                    territories | 'Write TO File' >> beam.WriteToText()\n",
    "                )\n",
    "#    p.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411c7a3",
   "metadata": {},
   "source": [
    "## DoFn Lifecycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326bda9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import AsSingleton, AsDict\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    # split territory into KV pair of (regionid, (territoryid, territoryname))\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "        \n",
    "                \n",
    "class LookupRegion(beam.DoFn):\n",
    "    def setup(self):\n",
    "        self.lookup = {1:'North', 2:'South', 3:'East', 4:'West'}\n",
    "        print('setup')\n",
    "        \n",
    "    def start_bundle(self):\n",
    "        print('start bundle')\n",
    "        \n",
    "    def process(self, element, uppercase = 0):\n",
    "        #lookuptable = {1:'North', 2:'South', 3:'East', 4:'West'}\n",
    "        territoryid, territoryname, regionid = element\n",
    "        region = self.lookup.get(regionid, 'No Region')\n",
    "        if uppercase == 1:\n",
    "            region = region.upper()\n",
    "        yield(territoryid, territoryname, regionid, region)\n",
    "        \n",
    "    def finish_bundle(self):\n",
    "        print('finish bundle')\n",
    "\n",
    "    def teardown(self):\n",
    "        print('teardown')\n",
    "        del self.lookup\n",
    "    \n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "          | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "    )\n",
    "    \n",
    "    lookup = (\n",
    "        territories\n",
    "        | beam.ParDo(LookupRegion(), uppercase = 1 ) \n",
    "        | 'Print Loopup' >> beam.Map(print)\n",
    "    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba3ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import AsList\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "\n",
    "class RegionParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid': int(regionid), 'regionname': regionname.title()}\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "        \n",
    "class LookupRegion(beam.DoFn):\n",
    "    def __init__(self):\n",
    "        print('init')\n",
    "        #self.lookup = {1:'north', 2:'south', 3:'east', 4:'west'}\n",
    "        #self.init_semaphore = False\n",
    "\n",
    "    def called_once(self, lookuptable):\n",
    "        print('called_once')\n",
    "        self.lookup = { e['regionid'] : e['regionname'] for e in lookuptable }\n",
    "        self.init_semaphore = False\n",
    "\n",
    "    def setup(self):\n",
    "        print('setup')\n",
    "        self.init_semaphore = True\n",
    "        #self.lookup = {1:'NORTH', 2:'South', 3:'East', 4:'West'}\n",
    "\n",
    "    def start_bundle(self):\n",
    "        print('start bundle')\n",
    "\n",
    "    def process(self, element, lookuptable = [{'regionid':1, 'regionname':'north'}, {'regionid':2, 'regionname':'south'}]):\n",
    "        if self.init_semaphore:\n",
    "            self.called_once(lookuptable)\n",
    "        territoryid, territoryname, regionid = element\n",
    "        yield(territoryid, territoryname, regionid, self.lookup.get(regionid, 'No Region'))\n",
    "\n",
    "    def finish_bundle(self):\n",
    "        print('finish bundle')\n",
    "\n",
    "    def teardown(self):\n",
    "        print('teardown')\n",
    "        del self.lookup\n",
    "        del self.init_semaphore\n",
    "                \n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "    regions = (\n",
    "        p | 'Read Regions' >> ReadFromText('regions.csv')\n",
    "          | 'Parse Regions' >> beam.ParDo(RegionParseDict())\n",
    "    )\n",
    "\n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "          | 'Spare Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "    )\n",
    "    \n",
    "    lookup = (\n",
    "        territories\n",
    "        | beam.ParDo(LookupRegion(), lookuptable = beam.pvalue.AsList(regions))\n",
    "        | 'Print Loopup' >> beam.Map(print)\n",
    "    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91df2171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be0f805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f866a6cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34bbd92f",
   "metadata": {},
   "source": [
    "## Code below is not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c24dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "from apache_beam.transforms.sql import SqlTransform\n",
    "#from collections import namedtuple\n",
    "from typing import NamedTuple\n",
    "from apache_beam import coders\n",
    "\n",
    "\n",
    "RegionSchema = namedtuple(\"RegionSchema\", (\"regionid\", \"regionname\"))\n",
    "#coders.registry.register_coder(RegionSchema, coders.RowCoder)\n",
    "class RegionSplitSchema(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield RegionSchema(int(regionid), regionname.title())\n",
    "\n",
    "#TerritorySchema = namedtuple(\"TerritorySchema\", (\"territoryid\", \"territoryname\", \"regionid\"))\n",
    "TerritorySchema = NamedTuple(\"TerritorySchema\", [(\"territoryid\", int), (\"territoryname\", str), (\"regionid\", int)])\n",
    "coders.registry.register_coder(TerritorySchema, coders.RowCoder)\n",
    "class TerritorySplitSchema(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield TerritorySchema(int(territoryid), territoryname.title(), int(regionid))\n",
    "\n",
    "        \n",
    "# class TerritorySplitNamedTuple(beam.DoFn):\n",
    "#     def process(self, element):\n",
    "#         territoryid, territoryname, regionid = element.split(',')\n",
    "#         yield {'territoryid':int(territoryid), 'territoryname' : territoryname, 'regionid':int(regionid)}\n",
    "\n",
    "regionsfilename = 'regions.csv'\n",
    "territoriesfilename = 'territories.csv'\n",
    "print('Start')\n",
    "with beam.Pipeline() as p:\n",
    "#     regions = (\n",
    "#               p | 'Read Regions' >> ReadFromText(regionsfilename)\n",
    "#                 | 'Split Regions' >> beam.ParDo(RegionSplitSchema()).with_output_types(RegionSchema)\n",
    "#               )\n",
    "        \n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Split Territories' >> beam.ParDo(TerritorySplitSchema()) #.with_output_types(TerritorySchema)\n",
    "#                    | beam.Map(lambda x: TerritorySchema(x.territoryid, x.territoryname, x.regionid)).with_output_types(TerritorySchema)\n",
    "#         Map(lambda x: PythonSchema(f_int32=x)).with_output_types(PythonSchema)\n",
    "#                     | 'Apply Territories Schema' >> beam.Map(lambda x : beam.Row(territoryid = int(x.territoryid)\n",
    "#                                                                                  , territoryname = str(x.territoryname)\n",
    "#                                                                                  , regionid = int(x.regionid)))\n",
    "#                    | 'Convert to Dictionary' >> beam.Map(lambda row : {\"regionid\" : row.regionid, \"territoryid\" : row.territoryid, \"territoryname\" : row.territoryname})\n",
    "#                     | SqlTransform(\"\"\"\n",
    "#                         SELECT regionid, territoryname as name, territoryid \n",
    "#                         FROM PCOLLECTION\n",
    "#                         \"\"\")\n",
    "#                    | 'Split Territories' >> beam.ParDo(TerritorySplitSchema()).with_output_types(TerritorySchema)\n",
    "#                     | SqlTransform(\"\"\"\n",
    "#                         SELECT regionid, count(*) as territories\n",
    "#                         FROM PCOLLECTION\n",
    "#                         GROUP BY regionID\n",
    "#                         ORDER BY territories DESC\n",
    "#                         \"\"\")\n",
    "#                    | 'Convert to dictionary' >> beam.Map(lambda row : {\"regionid\": row.regionid, \"territories\": row.territories})\n",
    "                    \n",
    "#             })\n",
    "                  )\n",
    "\n",
    "#     regions | 'Print regions' >> beam.Map(print)\n",
    "    territories | 'Print territories' >> beam.Map(print)\n",
    "\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d844f28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53300c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with beam.Pipeline() as pipeline:\n",
    "#     _ = (\n",
    "#         pipeline\n",
    "#         | beam.io.ReadFromPubSub(\n",
    "#             topic='projects/pubsub-public-data/topics/taxirides-realtime',\n",
    "#             timestamp_attribute=\"ts\").with_output_types(bytes)\n",
    "#         | \"Parse JSON payload\" >> beam.Map(json.loads)\n",
    "#         # Use beam.Row to create a schema-aware PCollection\n",
    "#         | \"Create beam Row\" >> beam.Map(\n",
    "#             lambda x: beam.Row(\n",
    "#                 ride_status=str(x['ride_status']),\n",
    "#                 passenger_count=int(x['passenger_count'])))\n",
    "#         # SqlTransform will computes result within an existing window\n",
    "#         | \"15s fixed windows\" >> beam.WindowInto(beam.window.FixedWindows(15))\n",
    "#         # Aggregate drop offs and pick ups that occur within each 15s window\n",
    "#         | SqlTransform(\n",
    "#             \"\"\"\n",
    "#              SELECT\n",
    "#                ride_status,\n",
    "#                COUNT(*) AS num_rides,\n",
    "#                SUM(passenger_count) AS total_passengers\n",
    "#              FROM PCOLLECTION\n",
    "#              WHERE NOT ride_status = 'enroute'\n",
    "#              GROUP BY ride_status\"\"\")\n",
    "#         # SqlTransform yields python objects with attributes corresponding to\n",
    "#         # the outputs of the query.\n",
    "#         # Collect those attributes, as well as window information, into a dict\n",
    "#         | \"Assemble Dictionary\" >> beam.Map(\n",
    "#             lambda row,\n",
    "#             window=beam.DoFn.WindowParam: {\n",
    "#                 \"ride_status\": row.ride_status,\n",
    "#                 \"num_rides\": row.num_rides,\n",
    "#                 \"total_passengers\": row.total_passengers,\n",
    "#                 \"window_start\": window.start.to_rfc3339(),\n",
    "#                 \"window_end\": window.end.to_rfc3339()\n",
    "#             })\n",
    "#         | \"Convert to JSON\" >> beam.Map(json.dumps)\n",
    "#         | \"UTF-8 encode\" >> beam.Map(lambda s: s.encode(\"utf-8\"))\n",
    "#         | beam.Map(print)\n",
    "#         #| beam.io.WriteToPubSub(topic=output_topic))\n",
    "#     )\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#   logging.getLogger().setLevel(logging.INFO)\n",
    "#   import argparse\n",
    "\n",
    "#   parser = argparse.ArgumentParser()\n",
    "#   parser.add_argument(\n",
    "#       '--output_topic',\n",
    "#       dest='output_topic',\n",
    "#       required=True,\n",
    "#       help=(\n",
    "#           'Cloud PubSub topic to write to (e.g. '\n",
    "#           'projects/my-project/topics/my-topic), must be created prior to '\n",
    "#           'running the pipeline.'))\n",
    "#   known_args, pipeline_args = parser.parse_known_args()\n",
    "\n",
    "#   run(known_args.output_topic, pipeline_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1393ae57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed99e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookuptable = [{'regionid':1, 'regionname':'North'}, {'regionid':2, 'regionname':'South'}]\n",
    "lookup = {e['regionid'] : e['regionname'] for e in lookuptable }\n",
    "print(lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35c6409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef64f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import AsIter, AsSingleton, AsList, AsDict\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "from apache_beam.io import ReadFromAvro, WriteToAvro\n",
    "from collections import namedtuple\n",
    "from apache_beam import coders\n",
    "from apache_beam.typehints.decorators import with_output_types\n",
    "\n",
    "\n",
    "class Region:\n",
    "    def __init__(self, regionid, regionname):\n",
    "        self.regionid = regionid\n",
    "        self.regionname = regionname\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'{self.regionid}|{self.regionname}'\n",
    "\n",
    "#     def encode(self, o):\n",
    "#         \"\"\"Encode to bytes with a trace that coder was used.\"\"\"\n",
    "#         # Our encoding prepends an 'x:' prefix.\n",
    "#         return b'x:%s' % o.encode('utf-8')\n",
    "\n",
    "#     def decode(self, s):\n",
    "#         # To decode, we strip off the prepended 'x:' prefix.\n",
    "#         s = s.decode('utf-8')\n",
    "#         #assert s[0:2] == 'x:'\n",
    "#         params = s[0:2].split('|')\n",
    "#         return Region(*params)\n",
    "\n",
    "#     def is_deterministic(self):\n",
    "#         # Since coded Player objects are used as keys below with\n",
    "#         # beam.CombinePerKey(sum), we require that this coder is deterministic\n",
    "#         # (i.e., two equivalent instances of the classes are encoded into the same\n",
    "#         # byte string) in order to guarantee consistent results.\n",
    "#         return True\n",
    "    \n",
    "class RegionSplitClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield Region(int(regionid), regionname.title())\n",
    "\n",
    "# class RegionCoder(coders.Coder):\n",
    "#   \"\"\"A custom coder for the RegionSchema\"\"\"\n",
    "#   def encode(self, o):\n",
    "#     \"\"\"Encode to bytes with a trace that coder was used.\"\"\"\n",
    "#     # Our encoding prepends an 'x:' prefix.\n",
    "#     return b'x:%s' % o.encode('utf-8')\n",
    "\n",
    "#   def decode(self, s):\n",
    "#     # To decode, we strip off the prepended 'x:' prefix.\n",
    "#     s = s.decode('utf-8')\n",
    "#     #assert s[0:2] == 'x:'\n",
    "#     params = s[0:2].split('|')\n",
    "#     return Region(*params)\n",
    "\n",
    "#   def is_deterministic(self):\n",
    "#     # Since coded Player objects are used as keys below with\n",
    "#     # beam.CombinePerKey(sum), we require that this coder is deterministic\n",
    "#     # (i.e., two equivalent instances of the classes are encoded into the same\n",
    "#     # byte string) in order to guarantee consistent results.\n",
    "#     return True\n",
    "# coders.registry.register_coder(Region, RegionCoder)\n",
    "\n",
    "# @with_output_types(typing.Tuple[Region, int])\n",
    "# def get_regions(descriptor):\n",
    "#   name, points = descriptor.split(',')\n",
    "#   return Player(name), int(points)\n",
    "\n",
    "\n",
    "# RegionSchema = namedtuple(\"RegionSchema\", (\"regionid\", \"regionname\"))\n",
    "# class RegionSplitSchema(beam.DoFn):\n",
    "#     def process(self, element):\n",
    "#         regionid, regionname = element.split(',')\n",
    "#         yield RegionSchema(int(regionid), regionname.title())\n",
    "\n",
    "class RegionSplitDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid': int(regionid), 'regionname': regionname.title()}\n",
    "\n",
    "\n",
    "class TerritorySplit(beam.DoFn):\n",
    "    # split territory into KV pair of (regionid, (territoryid, territoryname))\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "#        yield (int(regionid), (territoryid, territoryname.title())) \n",
    "        \n",
    "                \n",
    "def lookup_region(left, right):\n",
    "    territoryid, territoryname, regionid = left\n",
    "    yield territoryid, territoryname, regionid\n",
    "#    yield (territoryid, territorynme, regionid, right.get(regionid, 'No Region'))\n",
    "\n",
    "\n",
    "class LookupRegion(beam.DoFn):\n",
    "    def process(self, element, lookuptable = [{'regionid':1, 'regionname':'North'}, {'regionid':2, 'regionname':'South'}]):\n",
    "#        yield element\n",
    "        territoryid, territoryname, regionid = element\n",
    "        lookup = {e['regionid'] : e['regionname'] for e in lookuptable }\n",
    "        yield(territoryid, territoryname, regionid, lookup.get(regionid, 'No Region'))\n",
    "# #        yield (int(regionid), (territoryid, territoryname.title())) \n",
    "\n",
    "\n",
    "# def dummy(element):\n",
    "#     return element\n",
    "# #     regionid = element[0]\n",
    "# #     territoryid, territoryname = element[1]\n",
    "# #     return (territoryid, territoryname, regionid)\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read Regions' >> ReadFromText('regions.csv')\n",
    "          | 'Split Regions' >> beam.ParDo(RegionSplitDict())\n",
    "          #| 'Split Regions' >> beam.ParDo(RegionSplitClass())\n",
    "#          | 'Print Regions' >> beam.Map(print)\n",
    "    )\n",
    "\n",
    "#     regions = {1:\"North\", 2:\"South\", 3:\"East\", 4:\"West\"}\n",
    "#     regions = p | 'Create Regions' >> beam.Create([(1, 'North'), (2, 'South')])\n",
    "\n",
    "    \n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "          | 'Split Territories' >> beam.ParDo(TerritorySplit())\n",
    "#          | 'Print Territories' >> beam.Map(print)\n",
    "    )\n",
    "    \n",
    "    join = (\n",
    "        territories\n",
    "          #| 'Lookup Region' >> beam.Map(dummy)\n",
    "#          | 'Lookup Region' >> beam.Map(lookup_region, right = beam.pvalue.AsList(regions))\n",
    "#        | beam.ParDo(LookupRegion())\n",
    "        | beam.ParDo(LookupRegion(), lookuptable = beam.pvalue.AsList(regions))\n",
    "        | beam.Map(print)\n",
    "    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be59dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Beam 2.28.0 for Python 3",
   "language": "python",
   "name": "apache-beam-2.28.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
